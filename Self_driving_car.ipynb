{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Driving Car Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\compat\\compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import scipy\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from scipy import pi\n",
    "from itertools import islice\n",
    "import scipy.misc\n",
    "import cv2\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing data.txt\n"
     ]
    }
   ],
   "source": [
    "# read images and steering angles from driving_dataset folder\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from scipy import pi\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "\n",
    "DATA_FOLDER = './driving_dataset/' # change this to your folder\n",
    "TRAIN_FILE = os.path.join(DATA_FOLDER, 'data.txt')\n",
    "\n",
    "\n",
    "split =0.7\n",
    "X = []\n",
    "y = []\n",
    "with open(TRAIN_FILE) as fp:\n",
    "    for line in fp:\n",
    "        path, angle = line.strip().split()\n",
    "        full_path = os.path.join(DATA_FOLDER, path)\n",
    "        X.append(full_path)\n",
    "        \n",
    "        # converting angle from degrees to radians\n",
    "        y.append(float(angle) * pi / 180 )\n",
    "\n",
    "\n",
    "y = np.array(y)\n",
    "print(\"Completed processing data.txt\")\n",
    "\n",
    "split_index = int(len(y)*0.7)\n",
    "\n",
    "train_y = y[:split_index]\n",
    "test_y = y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "#points to the end of the last batch\n",
    "train_batch_pointer = 0\n",
    "val_batch_pointer = 0\n",
    "\n",
    "#read data.txt\n",
    "with open(\"driving_dataset/data.txt\") as f:\n",
    "    for line in f:\n",
    "        xs.append(\"driving_dataset/\" + line.split()[0])\n",
    "        #the paper by Nvidia uses the inverse of the turning radius,\n",
    "        #but steering wheel angle is proportional to the inverse of turning radius\n",
    "        #so the steering wheel angle in radians is used as the output\n",
    "        ys.append(float(line.split()[1]) * scipy.pi / 180)\n",
    "\n",
    "#get number of images\n",
    "num_images = len(xs)\n",
    "\n",
    "\n",
    "train_xs = xs[:int(len(xs) * 0.7)]\n",
    "train_ys = ys[:int(len(xs) * 0.7)]\n",
    "\n",
    "val_xs = xs[-int(len(xs) * 0.3):]\n",
    "val_ys = ys[-int(len(xs) * 0.3):]\n",
    "\n",
    "num_train_images = len(train_xs)\n",
    "num_val_images = len(val_xs)\n",
    "\n",
    "def LoadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(scipy.misc.imresize(scipy.misc.imread(train_xs[(train_batch_pointer + i) % num_train_images])[-150:], [66, 200]) / 255.0)\n",
    "        y_out.append([train_ys[(train_batch_pointer + i) % num_train_images]])\n",
    "    train_batch_pointer += batch_size\n",
    "    return x_out, y_out\n",
    "\n",
    "def LoadValBatch(batch_size):\n",
    "    global val_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(scipy.misc.imresize(scipy.misc.imread(val_xs[(val_batch_pointer + i) % num_val_images])[-150:], [66, 200]) / 255.0)\n",
    "        y_out.append([val_ys[(val_batch_pointer + i) % num_val_images]])\n",
    "    val_batch_pointer += batch_size\n",
    "    return x_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:5: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:6: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy;\n",
    "\n",
    "# PDF of train and test 'y' values. \n",
    "import matplotlib.pyplot as plt \n",
    "plt.hist(train_y, bins=50, normed=1, color='green', histtype ='step');\n",
    "plt.hist(test_y, bins=50, normed=1, color='red', histtype ='step');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-160811d01183>:55: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID')\n",
    "\n",
    "x_in = tf.placeholder(tf.float32, shape=[None, 66, 200, 3])\n",
    "y_tr = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "x_image = x_in\n",
    "\n",
    "#first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 24])\n",
    "b_conv1 = bias_variable([24])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 2) + b_conv1)\n",
    "\n",
    "#second convolutional layer\n",
    "W_conv2 = weight_variable([5, 5, 24, 36])\n",
    "b_conv2 = bias_variable([36])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2) + b_conv2)\n",
    "\n",
    "#third convolutional layer\n",
    "W_conv3 = weight_variable([5, 5, 36, 48])\n",
    "b_conv3 = bias_variable([48])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 2) + b_conv3)\n",
    "\n",
    "#fourth convolutional layer\n",
    "W_conv4 = weight_variable([3, 3, 48, 64])\n",
    "b_conv4 = bias_variable([64])\n",
    "\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4, 1) + b_conv4)\n",
    "\n",
    "#fifth convolutional layer\n",
    "W_conv5 = weight_variable([3, 3, 64, 64])\n",
    "b_conv5 = bias_variable([64])\n",
    "\n",
    "h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, 1) + b_conv5)\n",
    "\n",
    "#FCL 1\n",
    "W_fc1 = weight_variable([1152, 1164])\n",
    "b_fc1 = bias_variable([1164])\n",
    "\n",
    "h_conv5_flat = tf.reshape(h_conv5, [-1, 1152])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#FCL 2\n",
    "W_fc2 = weight_variable([1164, 100])\n",
    "b_fc2 = bias_variable([100])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "#FCL 3\n",
    "W_fc3 = weight_variable([100, 50])\n",
    "b_fc3 = bias_variable([50])\n",
    "\n",
    "h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "h_fc3_drop = tf.nn.dropout(h_fc3, keep_prob)\n",
    "\n",
    "#FCL 3\n",
    "W_fc4 = weight_variable([50, 10])\n",
    "b_fc4 = bias_variable([10])\n",
    "\n",
    "h_fc4 = tf.nn.relu(tf.matmul(h_fc3_drop, W_fc4) + b_fc4)\n",
    "\n",
    "h_fc4_drop = tf.nn.dropout(h_fc4, keep_prob)\n",
    "\n",
    "#Output\n",
    "W_fc5 = weight_variable([10, 1])\n",
    "b_fc5 = bias_variable([1])\n",
    "\n",
    "y_pred = tf.matmul(h_fc4_drop, W_fc5) + b_fc5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-image in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (0.16.2)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.0 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from scikit-image) (2.4)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from scikit-image) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from scikit-image) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from scikit-image) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from scikit-image) (5.2.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from scikit-image) (3.1.3)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from networkx>=2.0->scikit-image) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from PyWavelets>=0.4.0->scikit-image) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:38: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:38: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:48: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "C:\\Users\\hims1\\.conda\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:48: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 10.9201\n",
      "Epoch: 0, Step: 10, Loss: 6.81129\n",
      "Epoch: 0, Step: 20, Loss: 6.5184\n",
      "Epoch: 0, Step: 30, Loss: 6.39414\n",
      "Epoch: 0, Step: 40, Loss: 6.4146\n",
      "Epoch: 0, Step: 50, Loss: 6.17951\n",
      "Epoch: 0, Step: 60, Loss: 6.2861\n",
      "Epoch: 0, Step: 70, Loss: 6.46468\n",
      "Epoch: 0, Step: 80, Loss: 6.26695\n",
      "Epoch: 0, Step: 90, Loss: 5.93812\n",
      "Epoch: 0, Step: 100, Loss: 5.88701\n",
      "Epoch: 0, Step: 110, Loss: 5.80502\n",
      "Epoch: 0, Step: 120, Loss: 5.80508\n",
      "Epoch: 0, Step: 130, Loss: 6.1617\n",
      "Epoch: 0, Step: 140, Loss: 6.53484\n",
      "Epoch: 0, Step: 150, Loss: 5.94083\n",
      "Epoch: 0, Step: 160, Loss: 6.4883\n",
      "Epoch: 0, Step: 170, Loss: 5.87944\n",
      "Epoch: 0, Step: 180, Loss: 6.19864\n",
      "Epoch: 0, Step: 190, Loss: 5.92658\n",
      "Epoch: 0, Step: 200, Loss: 5.53121\n",
      "Epoch: 0, Step: 210, Loss: 5.46682\n",
      "Epoch: 0, Step: 220, Loss: 5.47873\n",
      "Epoch: 0, Step: 230, Loss: 5.50971\n",
      "Epoch: 0, Step: 240, Loss: 6.37759\n",
      "Epoch: 0, Step: 250, Loss: 5.54517\n",
      "Epoch: 0, Step: 260, Loss: 7.339\n",
      "Epoch: 0, Step: 270, Loss: 5.85403\n",
      "Epoch: 0, Step: 280, Loss: 6.41068\n",
      "Epoch: 0, Step: 290, Loss: 5.32308\n",
      "Epoch: 0, Step: 300, Loss: 5.20327\n",
      "Epoch: 0, Step: 310, Loss: 5.36468\n",
      "Epoch: 0, Step: 320, Loss: 6.34333\n",
      "Epoch: 0, Step: 330, Loss: 5.59854\n",
      "Epoch: 0, Step: 340, Loss: 5.33612\n",
      "Epoch: 0, Step: 350, Loss: 5.11114\n",
      "Epoch: 0, Step: 360, Loss: 5.13969\n",
      "Epoch: 0, Step: 370, Loss: 5.34961\n",
      "Epoch: 0, Step: 380, Loss: 5.2948\n",
      "Epoch: 0, Step: 390, Loss: 6.3576\n",
      "Epoch: 0, Step: 400, Loss: 5.75049\n",
      "Epoch: 0, Step: 410, Loss: 4.96259\n",
      "Epoch: 0, Step: 420, Loss: 4.92887\n",
      "Epoch: 0, Step: 430, Loss: 4.92686\n",
      "Epoch: 0, Step: 440, Loss: 4.9124\n",
      "Epoch: 0, Step: 450, Loss: 4.91248\n",
      "Epoch: 1, Step: 100, Loss: 5.3684\n",
      "Epoch: 1, Step: 110, Loss: 5.23238\n",
      "Epoch: 1, Step: 120, Loss: 4.89318\n",
      "Epoch: 1, Step: 130, Loss: 4.7846\n",
      "Epoch: 1, Step: 140, Loss: 4.76321\n",
      "Epoch: 1, Step: 150, Loss: 4.79575\n",
      "Epoch: 1, Step: 160, Loss: 5.15008\n",
      "Epoch: 1, Step: 170, Loss: 6.22876\n",
      "Epoch: 1, Step: 180, Loss: 5.46626\n",
      "Epoch: 1, Step: 190, Loss: 4.70417\n",
      "Epoch: 1, Step: 200, Loss: 4.63771\n",
      "Epoch: 1, Step: 210, Loss: 4.60629\n",
      "Epoch: 1, Step: 220, Loss: 4.58731\n",
      "Epoch: 1, Step: 230, Loss: 4.56698\n",
      "Epoch: 1, Step: 240, Loss: 4.58132\n",
      "Epoch: 1, Step: 250, Loss: 4.59816\n",
      "Epoch: 1, Step: 260, Loss: 4.50092\n",
      "Epoch: 1, Step: 270, Loss: 4.53173\n",
      "Epoch: 1, Step: 280, Loss: 4.57835\n",
      "Epoch: 1, Step: 290, Loss: 4.77321\n",
      "Epoch: 1, Step: 300, Loss: 4.60754\n",
      "Epoch: 1, Step: 310, Loss: 4.40407\n",
      "Epoch: 1, Step: 320, Loss: 4.3835\n",
      "Epoch: 1, Step: 330, Loss: 4.36581\n",
      "Epoch: 1, Step: 340, Loss: 4.3471\n",
      "Epoch: 1, Step: 350, Loss: 4.33308\n",
      "Epoch: 1, Step: 360, Loss: 6.12879\n",
      "Epoch: 1, Step: 370, Loss: 8.22156\n",
      "Epoch: 1, Step: 380, Loss: 4.28653\n",
      "Epoch: 1, Step: 390, Loss: 4.25837\n",
      "Epoch: 1, Step: 400, Loss: 4.23613\n",
      "Epoch: 1, Step: 410, Loss: 4.22227\n",
      "Epoch: 1, Step: 420, Loss: 4.20587\n",
      "Epoch: 1, Step: 430, Loss: 4.18979\n",
      "Epoch: 1, Step: 440, Loss: 4.17895\n",
      "Epoch: 1, Step: 450, Loss: 4.15965\n",
      "Epoch: 1, Step: 460, Loss: 4.14381\n",
      "Epoch: 1, Step: 470, Loss: 4.12162\n",
      "Epoch: 1, Step: 480, Loss: 4.10957\n",
      "Epoch: 1, Step: 490, Loss: 4.1047\n",
      "Epoch: 1, Step: 500, Loss: 4.08619\n",
      "Epoch: 1, Step: 510, Loss: 4.06129\n",
      "Epoch: 1, Step: 520, Loss: 4.03602\n",
      "Epoch: 1, Step: 530, Loss: 4.0221\n",
      "Epoch: 1, Step: 540, Loss: 4.00553\n",
      "Epoch: 1, Step: 550, Loss: 3.98823\n",
      "Epoch: 2, Step: 200, Loss: 3.98897\n",
      "Epoch: 2, Step: 210, Loss: 3.96559\n",
      "Epoch: 2, Step: 220, Loss: 3.95185\n",
      "Epoch: 2, Step: 230, Loss: 3.93957\n",
      "Epoch: 2, Step: 240, Loss: 3.9212\n",
      "Epoch: 2, Step: 250, Loss: 3.90709\n",
      "Epoch: 2, Step: 260, Loss: 3.89015\n",
      "Epoch: 2, Step: 270, Loss: 3.87709\n",
      "Epoch: 2, Step: 280, Loss: 3.8644\n",
      "Epoch: 2, Step: 290, Loss: 3.84676\n",
      "Epoch: 2, Step: 300, Loss: 3.87786\n",
      "Epoch: 2, Step: 310, Loss: 3.84063\n",
      "Epoch: 2, Step: 320, Loss: 7.99514\n",
      "Epoch: 2, Step: 330, Loss: 5.41914\n",
      "Epoch: 2, Step: 340, Loss: 3.77685\n",
      "Epoch: 2, Step: 350, Loss: 3.76212\n",
      "Epoch: 2, Step: 360, Loss: 3.7809\n",
      "Epoch: 2, Step: 370, Loss: 3.78398\n",
      "Epoch: 2, Step: 380, Loss: 3.72515\n",
      "Epoch: 2, Step: 390, Loss: 3.79858\n",
      "Epoch: 2, Step: 400, Loss: 3.79589\n",
      "Epoch: 2, Step: 410, Loss: 3.67318\n",
      "Epoch: 2, Step: 420, Loss: 3.71141\n",
      "Epoch: 2, Step: 430, Loss: 3.65606\n",
      "Epoch: 2, Step: 440, Loss: 3.6472\n",
      "Epoch: 2, Step: 450, Loss: 3.62324\n",
      "Epoch: 2, Step: 460, Loss: 3.71009\n",
      "Epoch: 2, Step: 470, Loss: 3.63332\n",
      "Epoch: 2, Step: 480, Loss: 3.71891\n",
      "Epoch: 2, Step: 490, Loss: 3.68213\n",
      "Epoch: 2, Step: 500, Loss: 3.56569\n",
      "Epoch: 2, Step: 510, Loss: 3.54424\n",
      "Epoch: 2, Step: 520, Loss: 3.53356\n",
      "Epoch: 2, Step: 530, Loss: 3.51883\n",
      "Epoch: 2, Step: 540, Loss: 3.50475\n",
      "Epoch: 2, Step: 550, Loss: 3.49431\n",
      "Epoch: 2, Step: 560, Loss: 3.479\n",
      "Epoch: 2, Step: 570, Loss: 3.46544\n",
      "Epoch: 2, Step: 580, Loss: 3.46212\n",
      "Epoch: 2, Step: 590, Loss: 3.44678\n",
      "Epoch: 2, Step: 600, Loss: 3.43363\n",
      "Epoch: 2, Step: 610, Loss: 3.41871\n",
      "Epoch: 2, Step: 620, Loss: 3.43494\n",
      "Epoch: 2, Step: 630, Loss: 3.42196\n",
      "Epoch: 2, Step: 640, Loss: 3.73462\n",
      "Epoch: 2, Step: 650, Loss: 3.37921\n",
      "Epoch: 3, Step: 300, Loss: 3.40417\n",
      "Epoch: 3, Step: 310, Loss: 3.4137\n",
      "Epoch: 3, Step: 320, Loss: 3.65905\n",
      "Epoch: 3, Step: 330, Loss: 3.39443\n",
      "Epoch: 3, Step: 340, Loss: 3.39719\n",
      "Epoch: 3, Step: 350, Loss: 3.60493\n",
      "Epoch: 3, Step: 360, Loss: 3.58283\n",
      "Epoch: 3, Step: 370, Loss: 3.28532\n",
      "Epoch: 3, Step: 380, Loss: 3.31643\n",
      "Epoch: 3, Step: 390, Loss: 3.2542\n",
      "Epoch: 3, Step: 400, Loss: 3.27224\n",
      "Epoch: 3, Step: 410, Loss: 3.31334\n",
      "Epoch: 3, Step: 420, Loss: 4.13499\n",
      "Epoch: 3, Step: 430, Loss: 3.37073\n",
      "Epoch: 3, Step: 440, Loss: 4.04264\n",
      "Epoch: 3, Step: 450, Loss: 3.68688\n",
      "Epoch: 3, Step: 460, Loss: 3.63451\n",
      "Epoch: 3, Step: 470, Loss: 3.6125\n",
      "Epoch: 3, Step: 480, Loss: 3.17738\n",
      "Epoch: 3, Step: 490, Loss: 3.14797\n",
      "Epoch: 3, Step: 500, Loss: 3.15725\n",
      "Epoch: 3, Step: 510, Loss: 3.26703\n",
      "Epoch: 3, Step: 520, Loss: 3.91682\n",
      "Epoch: 3, Step: 530, Loss: 3.42635\n",
      "Epoch: 3, Step: 540, Loss: 4.86723\n",
      "Epoch: 3, Step: 550, Loss: 3.72398\n",
      "Epoch: 3, Step: 560, Loss: 4.23898\n",
      "Epoch: 3, Step: 570, Loss: 3.2885\n",
      "Epoch: 3, Step: 580, Loss: 3.04546\n",
      "Epoch: 3, Step: 590, Loss: 3.11636\n",
      "Epoch: 3, Step: 600, Loss: 4.11766\n",
      "Epoch: 3, Step: 610, Loss: 3.80728\n",
      "Epoch: 3, Step: 620, Loss: 3.11483\n",
      "Epoch: 3, Step: 630, Loss: 3.0486\n",
      "Epoch: 3, Step: 640, Loss: 3.02155\n",
      "Epoch: 3, Step: 650, Loss: 3.34859\n",
      "Epoch: 3, Step: 660, Loss: 3.13057\n",
      "Epoch: 3, Step: 670, Loss: 4.00275\n",
      "Epoch: 3, Step: 680, Loss: 3.83577\n",
      "Epoch: 3, Step: 690, Loss: 3.07728\n",
      "Epoch: 3, Step: 700, Loss: 2.92893\n",
      "Epoch: 3, Step: 710, Loss: 2.92991\n",
      "Epoch: 3, Step: 720, Loss: 2.93423\n",
      "Epoch: 3, Step: 730, Loss: 2.93077\n",
      "Epoch: 3, Step: 740, Loss: 3.3312\n",
      "Epoch: 3, Step: 750, Loss: 3.4003\n",
      "Epoch: 4, Step: 400, Loss: 3.01328\n",
      "Epoch: 4, Step: 410, Loss: 2.87815\n",
      "Epoch: 4, Step: 420, Loss: 2.8629\n",
      "Epoch: 4, Step: 430, Loss: 2.8675\n",
      "Epoch: 4, Step: 440, Loss: 3.04709\n",
      "Epoch: 4, Step: 450, Loss: 4.14576\n",
      "Epoch: 4, Step: 460, Loss: 3.84897\n",
      "Epoch: 4, Step: 470, Loss: 2.84873\n",
      "Epoch: 4, Step: 480, Loss: 2.84734\n",
      "Epoch: 4, Step: 490, Loss: 2.77982\n",
      "Epoch: 4, Step: 500, Loss: 2.77376\n",
      "Epoch: 4, Step: 510, Loss: 2.76634\n",
      "Epoch: 4, Step: 520, Loss: 2.78337\n",
      "Epoch: 4, Step: 530, Loss: 2.82382\n",
      "Epoch: 4, Step: 540, Loss: 2.73982\n",
      "Epoch: 4, Step: 550, Loss: 2.74071\n",
      "Epoch: 4, Step: 560, Loss: 2.82008\n",
      "Epoch: 4, Step: 570, Loss: 3.05747\n",
      "Epoch: 4, Step: 580, Loss: 2.88269\n",
      "Epoch: 4, Step: 590, Loss: 2.69152\n",
      "Epoch: 4, Step: 600, Loss: 2.67631\n",
      "Epoch: 4, Step: 610, Loss: 2.66771\n",
      "Epoch: 4, Step: 620, Loss: 2.65904\n",
      "Epoch: 4, Step: 630, Loss: 2.65249\n",
      "Epoch: 4, Step: 640, Loss: 3.17514\n",
      "Epoch: 4, Step: 650, Loss: 7.6607\n",
      "Epoch: 4, Step: 660, Loss: 2.6675\n",
      "Epoch: 4, Step: 670, Loss: 2.61681\n",
      "Epoch: 4, Step: 680, Loss: 2.60352\n",
      "Epoch: 4, Step: 690, Loss: 2.59708\n",
      "Epoch: 4, Step: 700, Loss: 2.58843\n",
      "Epoch: 4, Step: 710, Loss: 2.57792\n",
      "Epoch: 4, Step: 720, Loss: 2.57467\n",
      "Epoch: 4, Step: 730, Loss: 2.5635\n",
      "Epoch: 4, Step: 740, Loss: 2.55505\n",
      "Epoch: 4, Step: 750, Loss: 2.5453\n",
      "Epoch: 4, Step: 760, Loss: 2.53859\n",
      "Epoch: 4, Step: 770, Loss: 2.54471\n",
      "Epoch: 4, Step: 780, Loss: 2.53681\n",
      "Epoch: 4, Step: 790, Loss: 2.51767\n",
      "Epoch: 4, Step: 800, Loss: 2.5015\n",
      "Epoch: 4, Step: 810, Loss: 2.49136\n",
      "Epoch: 4, Step: 820, Loss: 2.48499\n",
      "Epoch: 4, Step: 830, Loss: 2.47796\n",
      "Epoch: 4, Step: 840, Loss: 2.46799\n",
      "Epoch: 4, Step: 850, Loss: 2.46291\n",
      "Epoch: 5, Step: 500, Loss: 2.45814\n",
      "Epoch: 5, Step: 510, Loss: 2.45541\n",
      "Epoch: 5, Step: 520, Loss: 2.4439\n",
      "Epoch: 5, Step: 530, Loss: 2.43754\n",
      "Epoch: 5, Step: 540, Loss: 2.42743\n",
      "Epoch: 5, Step: 550, Loss: 2.41922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Step: 560, Loss: 2.40778\n",
      "Epoch: 5, Step: 570, Loss: 2.40083\n",
      "Epoch: 5, Step: 580, Loss: 2.4084\n",
      "Epoch: 5, Step: 590, Loss: 2.44225\n",
      "Epoch: 5, Step: 600, Loss: 5.22552\n",
      "Epoch: 5, Step: 610, Loss: 5.16272\n",
      "Epoch: 5, Step: 620, Loss: 2.35829\n",
      "Epoch: 5, Step: 630, Loss: 2.3481\n",
      "Epoch: 5, Step: 640, Loss: 2.38421\n",
      "Epoch: 5, Step: 650, Loss: 2.37711\n",
      "Epoch: 5, Step: 660, Loss: 2.36217\n",
      "Epoch: 5, Step: 670, Loss: 2.37328\n",
      "Epoch: 5, Step: 680, Loss: 2.43044\n",
      "Epoch: 5, Step: 690, Loss: 2.30046\n",
      "Epoch: 5, Step: 700, Loss: 2.31855\n",
      "Epoch: 5, Step: 710, Loss: 2.30669\n",
      "Epoch: 5, Step: 720, Loss: 2.29149\n",
      "Epoch: 5, Step: 730, Loss: 2.27742\n",
      "Epoch: 5, Step: 740, Loss: 2.33281\n",
      "Epoch: 5, Step: 750, Loss: 2.3187\n",
      "Epoch: 5, Step: 760, Loss: 2.3282\n",
      "Epoch: 5, Step: 770, Loss: 2.32985\n",
      "Epoch: 5, Step: 780, Loss: 2.25259\n",
      "Epoch: 5, Step: 790, Loss: 2.23132\n",
      "Epoch: 5, Step: 800, Loss: 2.22236\n",
      "Epoch: 5, Step: 810, Loss: 2.20926\n",
      "Epoch: 5, Step: 820, Loss: 2.20256\n",
      "Epoch: 5, Step: 830, Loss: 2.19543\n",
      "Epoch: 5, Step: 840, Loss: 2.18989\n",
      "Epoch: 5, Step: 850, Loss: 2.17984\n",
      "Epoch: 5, Step: 860, Loss: 2.17905\n",
      "Epoch: 5, Step: 870, Loss: 2.16807\n",
      "Epoch: 5, Step: 880, Loss: 2.16255\n",
      "Epoch: 5, Step: 890, Loss: 2.15181\n",
      "Epoch: 5, Step: 900, Loss: 2.15835\n",
      "Epoch: 5, Step: 910, Loss: 2.17689\n",
      "Epoch: 5, Step: 920, Loss: 2.4378\n",
      "Epoch: 5, Step: 930, Loss: 2.17571\n",
      "Epoch: 5, Step: 940, Loss: 2.1374\n",
      "Epoch: 5, Step: 950, Loss: 2.1766\n",
      "Epoch: 6, Step: 600, Loss: 2.35666\n",
      "Epoch: 6, Step: 610, Loss: 2.25251\n",
      "Epoch: 6, Step: 620, Loss: 2.11168\n",
      "Epoch: 6, Step: 630, Loss: 2.33671\n",
      "Epoch: 6, Step: 640, Loss: 2.38357\n",
      "Epoch: 6, Step: 650, Loss: 2.08016\n",
      "Epoch: 6, Step: 660, Loss: 2.11108\n",
      "Epoch: 6, Step: 670, Loss: 2.04952\n",
      "Epoch: 6, Step: 680, Loss: 2.07051\n",
      "Epoch: 6, Step: 690, Loss: 2.0485\n",
      "Epoch: 6, Step: 700, Loss: 2.80331\n",
      "Epoch: 6, Step: 710, Loss: 2.18125\n",
      "Epoch: 6, Step: 720, Loss: 2.7413\n",
      "Epoch: 6, Step: 730, Loss: 2.60229\n",
      "Epoch: 6, Step: 740, Loss: 2.22174\n",
      "Epoch: 6, Step: 750, Loss: 2.53501\n",
      "Epoch: 6, Step: 760, Loss: 2.10025\n",
      "Epoch: 6, Step: 770, Loss: 1.99602\n",
      "Epoch: 6, Step: 780, Loss: 1.97866\n",
      "Epoch: 6, Step: 790, Loss: 2.07022\n",
      "Epoch: 6, Step: 800, Loss: 2.4521\n",
      "Epoch: 6, Step: 810, Loss: 2.65706\n",
      "Epoch: 6, Step: 820, Loss: 3.13807\n",
      "Epoch: 6, Step: 830, Loss: 3.03067\n",
      "Epoch: 6, Step: 840, Loss: 2.96657\n",
      "Epoch: 6, Step: 850, Loss: 2.32751\n",
      "Epoch: 6, Step: 860, Loss: 1.91968\n",
      "Epoch: 6, Step: 870, Loss: 1.93403\n",
      "Epoch: 6, Step: 880, Loss: 2.74841\n",
      "Epoch: 6, Step: 890, Loss: 3.09046\n",
      "Epoch: 6, Step: 900, Loss: 1.95003\n",
      "Epoch: 6, Step: 910, Loss: 1.96816\n",
      "Epoch: 6, Step: 920, Loss: 1.88725\n",
      "Epoch: 6, Step: 930, Loss: 2.16731\n",
      "Epoch: 6, Step: 940, Loss: 2.06574\n",
      "Epoch: 6, Step: 950, Loss: 2.6524\n",
      "Epoch: 6, Step: 960, Loss: 2.90015\n",
      "Epoch: 6, Step: 970, Loss: 2.10273\n",
      "Epoch: 6, Step: 980, Loss: 1.84063\n",
      "Epoch: 6, Step: 990, Loss: 1.84628\n",
      "Epoch: 6, Step: 1000, Loss: 1.84806\n",
      "Epoch: 6, Step: 1010, Loss: 1.86082\n",
      "Epoch: 6, Step: 1020, Loss: 2.12397\n",
      "Epoch: 6, Step: 1030, Loss: 2.31759\n",
      "Epoch: 6, Step: 1040, Loss: 2.01544\n",
      "Epoch: 6, Step: 1050, Loss: 1.84983\n",
      "Epoch: 7, Step: 700, Loss: 1.81837\n",
      "Epoch: 7, Step: 710, Loss: 1.79626\n",
      "Epoch: 7, Step: 720, Loss: 1.90005\n",
      "Epoch: 7, Step: 730, Loss: 2.76983\n",
      "Epoch: 7, Step: 740, Loss: 3.09816\n",
      "Epoch: 7, Step: 750, Loss: 1.87556\n",
      "Epoch: 7, Step: 760, Loss: 1.82508\n",
      "Epoch: 7, Step: 770, Loss: 1.74231\n",
      "Epoch: 7, Step: 780, Loss: 1.75001\n",
      "Epoch: 7, Step: 790, Loss: 1.73813\n",
      "Epoch: 7, Step: 800, Loss: 1.74779\n",
      "Epoch: 7, Step: 810, Loss: 1.76986\n",
      "Epoch: 7, Step: 820, Loss: 1.74715\n",
      "Epoch: 7, Step: 830, Loss: 1.71244\n",
      "Epoch: 7, Step: 840, Loss: 1.78618\n",
      "Epoch: 7, Step: 850, Loss: 2.08816\n",
      "Epoch: 7, Step: 860, Loss: 1.84877\n",
      "Epoch: 7, Step: 870, Loss: 1.81883\n",
      "Epoch: 7, Step: 880, Loss: 1.68253\n",
      "Epoch: 7, Step: 890, Loss: 1.67023\n",
      "Epoch: 7, Step: 900, Loss: 1.66234\n",
      "Epoch: 7, Step: 910, Loss: 1.65694\n",
      "Epoch: 7, Step: 920, Loss: 1.70688\n",
      "Epoch: 7, Step: 930, Loss: 6.97269\n",
      "Epoch: 7, Step: 940, Loss: 1.82824\n",
      "Epoch: 7, Step: 950, Loss: 1.63437\n",
      "Epoch: 7, Step: 960, Loss: 1.62293\n",
      "Epoch: 7, Step: 970, Loss: 1.62088\n",
      "Epoch: 7, Step: 980, Loss: 1.61843\n",
      "Epoch: 7, Step: 990, Loss: 1.60704\n",
      "Epoch: 7, Step: 1000, Loss: 1.61899\n",
      "Epoch: 7, Step: 1010, Loss: 1.60129\n",
      "Epoch: 7, Step: 1020, Loss: 1.59548\n",
      "Epoch: 7, Step: 1030, Loss: 1.60733\n",
      "Epoch: 7, Step: 1040, Loss: 1.60928\n",
      "Epoch: 7, Step: 1050, Loss: 1.5815\n",
      "Epoch: 7, Step: 1060, Loss: 1.58035\n",
      "Epoch: 7, Step: 1070, Loss: 1.59453\n",
      "Epoch: 7, Step: 1080, Loss: 1.55634\n",
      "Epoch: 7, Step: 1090, Loss: 1.54648\n",
      "Epoch: 7, Step: 1100, Loss: 1.54862\n",
      "Epoch: 7, Step: 1110, Loss: 1.53923\n",
      "Epoch: 7, Step: 1120, Loss: 1.53148\n",
      "Epoch: 7, Step: 1130, Loss: 1.53048\n",
      "Epoch: 7, Step: 1140, Loss: 1.5217\n",
      "Epoch: 7, Step: 1150, Loss: 1.5242\n",
      "Epoch: 8, Step: 800, Loss: 1.52793\n",
      "Epoch: 8, Step: 810, Loss: 1.51292\n",
      "Epoch: 8, Step: 820, Loss: 1.49826\n",
      "Epoch: 8, Step: 830, Loss: 1.49411\n",
      "Epoch: 8, Step: 840, Loss: 1.48594\n",
      "Epoch: 8, Step: 850, Loss: 1.48123\n",
      "Epoch: 8, Step: 860, Loss: 1.48237\n",
      "Epoch: 8, Step: 870, Loss: 1.5354\n",
      "Epoch: 8, Step: 880, Loss: 2.95681\n",
      "Epoch: 8, Step: 890, Loss: 5.61155\n",
      "Epoch: 8, Step: 900, Loss: 1.45732\n",
      "Epoch: 8, Step: 910, Loss: 1.4444\n",
      "Epoch: 8, Step: 920, Loss: 1.47982\n",
      "Epoch: 8, Step: 930, Loss: 1.46529\n",
      "Epoch: 8, Step: 940, Loss: 1.47394\n",
      "Epoch: 8, Step: 950, Loss: 1.44728\n",
      "Epoch: 8, Step: 960, Loss: 1.53105\n",
      "Epoch: 8, Step: 970, Loss: 1.4264\n",
      "Epoch: 8, Step: 980, Loss: 1.43921\n",
      "Epoch: 8, Step: 990, Loss: 1.43488\n",
      "Epoch: 8, Step: 1000, Loss: 1.44215\n",
      "Epoch: 8, Step: 1010, Loss: 1.41017\n",
      "Epoch: 8, Step: 1020, Loss: 1.41858\n",
      "Epoch: 8, Step: 1030, Loss: 1.42863\n",
      "Epoch: 8, Step: 1040, Loss: 1.46736\n",
      "Epoch: 8, Step: 1050, Loss: 1.46378\n",
      "Epoch: 8, Step: 1060, Loss: 1.38401\n",
      "Epoch: 8, Step: 1070, Loss: 1.38026\n",
      "Epoch: 8, Step: 1080, Loss: 1.35475\n",
      "Epoch: 8, Step: 1090, Loss: 1.34895\n",
      "Epoch: 8, Step: 1100, Loss: 1.3502\n",
      "Epoch: 8, Step: 1110, Loss: 1.34283\n",
      "Epoch: 8, Step: 1120, Loss: 1.33216\n",
      "Epoch: 8, Step: 1130, Loss: 1.32945\n",
      "Epoch: 8, Step: 1140, Loss: 1.33378\n",
      "Epoch: 8, Step: 1150, Loss: 1.32771\n",
      "Epoch: 8, Step: 1160, Loss: 1.32734\n",
      "Epoch: 8, Step: 1170, Loss: 1.30416\n",
      "Epoch: 8, Step: 1180, Loss: 1.3132\n",
      "Epoch: 8, Step: 1190, Loss: 1.35712\n",
      "Epoch: 8, Step: 1200, Loss: 1.48858\n",
      "Epoch: 8, Step: 1210, Loss: 1.42712\n",
      "Epoch: 8, Step: 1220, Loss: 1.2967\n",
      "Epoch: 8, Step: 1230, Loss: 1.35252\n",
      "Epoch: 8, Step: 1240, Loss: 1.44262\n",
      "Epoch: 8, Step: 1250, Loss: 1.53592\n",
      "Epoch: 9, Step: 900, Loss: 1.26375\n",
      "Epoch: 9, Step: 910, Loss: 1.41806\n",
      "Epoch: 9, Step: 920, Loss: 1.52767\n",
      "Epoch: 9, Step: 930, Loss: 1.27723\n",
      "Epoch: 9, Step: 940, Loss: 1.29546\n",
      "Epoch: 9, Step: 950, Loss: 1.23702\n",
      "Epoch: 9, Step: 960, Loss: 1.24252\n",
      "Epoch: 9, Step: 970, Loss: 1.2516\n",
      "Epoch: 9, Step: 980, Loss: 1.889\n",
      "Epoch: 9, Step: 990, Loss: 1.49525\n",
      "Epoch: 9, Step: 1000, Loss: 1.72471\n",
      "Epoch: 9, Step: 1010, Loss: 1.84713\n",
      "Epoch: 9, Step: 1020, Loss: 1.30947\n",
      "Epoch: 9, Step: 1030, Loss: 1.79922\n",
      "Epoch: 9, Step: 1040, Loss: 1.3227\n",
      "Epoch: 9, Step: 1050, Loss: 1.18936\n",
      "Epoch: 9, Step: 1060, Loss: 1.18861\n",
      "Epoch: 9, Step: 1070, Loss: 1.28904\n",
      "Epoch: 9, Step: 1080, Loss: 1.41464\n",
      "Epoch: 9, Step: 1090, Loss: 2.08852\n",
      "Epoch: 9, Step: 1100, Loss: 1.8757\n",
      "Epoch: 9, Step: 1110, Loss: 2.84196\n",
      "Epoch: 9, Step: 1120, Loss: 1.85603\n",
      "Epoch: 9, Step: 1130, Loss: 1.76469\n",
      "Epoch: 9, Step: 1140, Loss: 1.14158\n",
      "Epoch: 9, Step: 1150, Loss: 1.13876\n",
      "Epoch: 9, Step: 1160, Loss: 1.75367\n",
      "Epoch: 9, Step: 1170, Loss: 2.56401\n",
      "Epoch: 9, Step: 1180, Loss: 1.18719\n",
      "Epoch: 9, Step: 1190, Loss: 1.22062\n",
      "Epoch: 9, Step: 1200, Loss: 1.11167\n",
      "Epoch: 9, Step: 1210, Loss: 1.30664\n",
      "Epoch: 9, Step: 1220, Loss: 1.41545\n",
      "Epoch: 9, Step: 1230, Loss: 1.55938\n",
      "Epoch: 9, Step: 1240, Loss: 2.21875\n",
      "Epoch: 9, Step: 1250, Loss: 1.40485\n",
      "Epoch: 9, Step: 1260, Loss: 1.08291\n",
      "Epoch: 9, Step: 1270, Loss: 1.08555\n",
      "Epoch: 9, Step: 1280, Loss: 1.1002\n",
      "Epoch: 9, Step: 1290, Loss: 1.14835\n",
      "Epoch: 9, Step: 1300, Loss: 1.27021\n",
      "Epoch: 9, Step: 1310, Loss: 1.47888\n",
      "Epoch: 9, Step: 1320, Loss: 1.25266\n",
      "Epoch: 9, Step: 1330, Loss: 1.13064\n",
      "Epoch: 9, Step: 1340, Loss: 1.05867\n",
      "Epoch: 9, Step: 1350, Loss: 1.04339\n",
      "Epoch: 10, Step: 1000, Loss: 1.1283\n",
      "Epoch: 10, Step: 1010, Loss: 1.75304\n",
      "Epoch: 10, Step: 1020, Loss: 2.35307\n",
      "Epoch: 10, Step: 1030, Loss: 1.23417\n",
      "Epoch: 10, Step: 1040, Loss: 1.10933\n",
      "Epoch: 10, Step: 1050, Loss: 1.01832\n",
      "Epoch: 10, Step: 1060, Loss: 1.01653\n",
      "Epoch: 10, Step: 1070, Loss: 1.00576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Step: 1080, Loss: 1.01241\n",
      "Epoch: 10, Step: 1090, Loss: 1.03245\n",
      "Epoch: 10, Step: 1100, Loss: 1.03612\n",
      "Epoch: 10, Step: 1110, Loss: 0.991705\n",
      "Epoch: 10, Step: 1120, Loss: 1.08238\n",
      "Epoch: 10, Step: 1130, Loss: 1.36692\n",
      "Epoch: 10, Step: 1140, Loss: 1.06748\n",
      "Epoch: 10, Step: 1150, Loss: 1.14716\n",
      "Epoch: 10, Step: 1160, Loss: 0.969216\n",
      "Epoch: 10, Step: 1170, Loss: 0.966201\n",
      "Epoch: 10, Step: 1180, Loss: 0.96027\n",
      "Epoch: 10, Step: 1190, Loss: 0.957529\n",
      "Epoch: 10, Step: 1200, Loss: 0.978186\n",
      "Epoch: 10, Step: 1210, Loss: 5.48654\n",
      "Epoch: 10, Step: 1220, Loss: 1.85965\n",
      "Epoch: 10, Step: 1230, Loss: 0.941574\n",
      "Epoch: 10, Step: 1240, Loss: 0.946644\n",
      "Epoch: 10, Step: 1250, Loss: 0.957652\n",
      "Epoch: 10, Step: 1260, Loss: 0.938067\n",
      "Epoch: 10, Step: 1270, Loss: 0.94762\n",
      "Epoch: 10, Step: 1280, Loss: 0.977377\n",
      "Epoch: 10, Step: 1290, Loss: 0.929985\n",
      "Epoch: 10, Step: 1300, Loss: 0.928106\n",
      "Epoch: 10, Step: 1310, Loss: 0.938211\n",
      "Epoch: 10, Step: 1320, Loss: 0.93218\n",
      "Epoch: 10, Step: 1330, Loss: 0.901893\n",
      "Epoch: 10, Step: 1340, Loss: 0.910724\n",
      "Epoch: 10, Step: 1350, Loss: 0.899314\n",
      "Epoch: 10, Step: 1360, Loss: 0.883609\n",
      "Epoch: 10, Step: 1370, Loss: 0.875378\n",
      "Epoch: 10, Step: 1380, Loss: 0.8732\n",
      "Epoch: 10, Step: 1390, Loss: 0.865957\n",
      "Epoch: 10, Step: 1400, Loss: 0.860773\n",
      "Epoch: 10, Step: 1410, Loss: 0.859003\n",
      "Epoch: 10, Step: 1420, Loss: 0.852281\n",
      "Epoch: 10, Step: 1430, Loss: 0.850047\n",
      "Epoch: 10, Step: 1440, Loss: 0.849026\n",
      "Epoch: 10, Step: 1450, Loss: 0.843227\n",
      "Epoch: 11, Step: 1100, Loss: 0.840464\n",
      "Epoch: 11, Step: 1110, Loss: 0.836574\n",
      "Epoch: 11, Step: 1120, Loss: 0.833116\n",
      "Epoch: 11, Step: 1130, Loss: 0.829085\n",
      "Epoch: 11, Step: 1140, Loss: 0.828608\n",
      "Epoch: 11, Step: 1150, Loss: 0.898848\n",
      "Epoch: 11, Step: 1160, Loss: 1.0152\n",
      "Epoch: 11, Step: 1170, Loss: 6.07794\n",
      "Epoch: 11, Step: 1180, Loss: 0.823002\n",
      "Epoch: 11, Step: 1190, Loss: 0.845421\n",
      "Epoch: 11, Step: 1200, Loss: 0.833518\n",
      "Epoch: 11, Step: 1210, Loss: 0.827034\n",
      "Epoch: 11, Step: 1220, Loss: 0.824986\n",
      "Epoch: 11, Step: 1230, Loss: 0.804579\n",
      "Epoch: 11, Step: 1240, Loss: 1.01933\n",
      "Epoch: 11, Step: 1250, Loss: 0.831387\n",
      "Epoch: 11, Step: 1260, Loss: 0.782862\n",
      "Epoch: 11, Step: 1270, Loss: 0.865143\n",
      "Epoch: 11, Step: 1280, Loss: 0.775871\n",
      "Epoch: 11, Step: 1290, Loss: 0.785334\n",
      "Epoch: 11, Step: 1300, Loss: 0.794005\n",
      "Epoch: 11, Step: 1310, Loss: 0.861217\n",
      "Epoch: 11, Step: 1320, Loss: 0.811831\n",
      "Epoch: 11, Step: 1330, Loss: 0.892445\n",
      "Epoch: 11, Step: 1340, Loss: 0.805505\n",
      "Epoch: 11, Step: 1350, Loss: 0.77195\n",
      "Epoch: 11, Step: 1360, Loss: 0.755553\n",
      "Epoch: 11, Step: 1370, Loss: 0.747077\n",
      "Epoch: 11, Step: 1380, Loss: 0.746143\n",
      "Epoch: 11, Step: 1390, Loss: 0.743878\n",
      "Epoch: 11, Step: 1400, Loss: 0.740149\n",
      "Epoch: 11, Step: 1410, Loss: 0.735269\n",
      "Epoch: 11, Step: 1420, Loss: 0.736597\n",
      "Epoch: 11, Step: 1430, Loss: 0.734198\n",
      "Epoch: 11, Step: 1440, Loss: 0.735813\n",
      "Epoch: 11, Step: 1450, Loss: 0.716013\n",
      "Epoch: 11, Step: 1460, Loss: 0.718204\n",
      "Epoch: 11, Step: 1470, Loss: 0.750008\n",
      "Epoch: 11, Step: 1480, Loss: 0.78201\n",
      "Epoch: 11, Step: 1490, Loss: 1.02251\n",
      "Epoch: 11, Step: 1500, Loss: 0.711855\n",
      "Epoch: 11, Step: 1510, Loss: 0.748078\n",
      "Epoch: 11, Step: 1520, Loss: 0.743887\n",
      "Epoch: 11, Step: 1530, Loss: 0.877418\n",
      "Epoch: 11, Step: 1540, Loss: 0.723183\n",
      "Epoch: 11, Step: 1550, Loss: 0.828821\n",
      "Epoch: 12, Step: 1200, Loss: 0.971642\n",
      "Epoch: 12, Step: 1210, Loss: 0.802484\n",
      "Epoch: 12, Step: 1220, Loss: 0.71431\n",
      "Epoch: 12, Step: 1230, Loss: 0.672891\n",
      "Epoch: 12, Step: 1240, Loss: 0.67465\n",
      "Epoch: 12, Step: 1250, Loss: 0.676628\n",
      "Epoch: 12, Step: 1260, Loss: 1.17437\n",
      "Epoch: 12, Step: 1270, Loss: 1.12154\n",
      "Epoch: 12, Step: 1280, Loss: 1.15819\n",
      "Epoch: 12, Step: 1290, Loss: 1.53842\n",
      "Epoch: 12, Step: 1300, Loss: 0.803265\n",
      "Epoch: 12, Step: 1310, Loss: 1.27914\n",
      "Epoch: 12, Step: 1320, Loss: 0.856125\n",
      "Epoch: 12, Step: 1330, Loss: 0.642694\n",
      "Epoch: 12, Step: 1340, Loss: 0.653285\n",
      "Epoch: 12, Step: 1350, Loss: 0.716163\n",
      "Epoch: 12, Step: 1360, Loss: 0.786621\n",
      "Epoch: 12, Step: 1370, Loss: 1.60956\n",
      "Epoch: 12, Step: 1380, Loss: 0.955711\n",
      "Epoch: 12, Step: 1390, Loss: 2.74798\n",
      "Epoch: 12, Step: 1400, Loss: 1.13241\n",
      "Epoch: 12, Step: 1410, Loss: 1.49321\n",
      "Epoch: 12, Step: 1420, Loss: 0.646251\n",
      "Epoch: 12, Step: 1430, Loss: 0.613249\n",
      "Epoch: 12, Step: 1440, Loss: 0.88707\n",
      "Epoch: 12, Step: 1450, Loss: 1.96288\n",
      "Epoch: 12, Step: 1460, Loss: 0.935622\n",
      "Epoch: 12, Step: 1470, Loss: 0.714207\n",
      "Epoch: 12, Step: 1480, Loss: 0.666843\n",
      "Epoch: 12, Step: 1490, Loss: 0.694464\n",
      "Epoch: 12, Step: 1500, Loss: 0.966682\n",
      "Epoch: 12, Step: 1510, Loss: 0.912539\n",
      "Epoch: 12, Step: 1520, Loss: 1.85174\n",
      "Epoch: 12, Step: 1530, Loss: 1.07521\n",
      "Epoch: 12, Step: 1540, Loss: 0.577451\n",
      "Epoch: 12, Step: 1550, Loss: 0.584292\n",
      "Epoch: 12, Step: 1560, Loss: 0.593812\n",
      "Epoch: 12, Step: 1570, Loss: 0.611738\n",
      "Epoch: 12, Step: 1580, Loss: 0.717744\n",
      "Epoch: 12, Step: 1590, Loss: 1.16913\n",
      "Epoch: 12, Step: 1600, Loss: 0.960303\n",
      "Epoch: 12, Step: 1610, Loss: 0.661656\n",
      "Epoch: 12, Step: 1620, Loss: 0.585926\n",
      "Epoch: 12, Step: 1630, Loss: 0.561859\n",
      "Epoch: 12, Step: 1640, Loss: 0.615392\n",
      "Epoch: 12, Step: 1650, Loss: 1.06369\n",
      "Epoch: 13, Step: 1300, Loss: 1.89009\n",
      "Epoch: 13, Step: 1310, Loss: 0.986314\n",
      "Epoch: 13, Step: 1320, Loss: 0.64851\n",
      "Epoch: 13, Step: 1330, Loss: 0.543806\n",
      "Epoch: 13, Step: 1340, Loss: 0.539882\n",
      "Epoch: 13, Step: 1350, Loss: 0.533069\n",
      "Epoch: 13, Step: 1360, Loss: 0.535112\n",
      "Epoch: 13, Step: 1370, Loss: 0.55569\n",
      "Epoch: 13, Step: 1380, Loss: 0.572484\n",
      "Epoch: 13, Step: 1390, Loss: 0.543651\n",
      "Epoch: 13, Step: 1400, Loss: 0.612739\n",
      "Epoch: 13, Step: 1410, Loss: 0.783319\n",
      "Epoch: 13, Step: 1420, Loss: 0.746014\n",
      "Epoch: 13, Step: 1430, Loss: 0.860438\n",
      "Epoch: 13, Step: 1440, Loss: 0.559504\n",
      "Epoch: 13, Step: 1450, Loss: 0.571784\n",
      "Epoch: 13, Step: 1460, Loss: 0.512411\n",
      "Epoch: 13, Step: 1470, Loss: 0.509734\n",
      "Epoch: 13, Step: 1480, Loss: 0.510606\n",
      "Epoch: 13, Step: 1490, Loss: 4.27457\n",
      "Epoch: 13, Step: 1500, Loss: 3.04177\n",
      "Epoch: 13, Step: 1510, Loss: 0.502234\n",
      "Epoch: 13, Step: 1520, Loss: 0.493935\n",
      "Epoch: 13, Step: 1530, Loss: 0.489007\n",
      "Epoch: 13, Step: 1540, Loss: 0.49584\n",
      "Epoch: 13, Step: 1550, Loss: 0.487363\n",
      "Epoch: 13, Step: 1560, Loss: 0.5067\n",
      "Epoch: 13, Step: 1570, Loss: 0.508784\n",
      "Epoch: 13, Step: 1580, Loss: 0.48651\n",
      "Epoch: 13, Step: 1590, Loss: 0.479056\n",
      "Epoch: 13, Step: 1600, Loss: 0.474363\n",
      "Epoch: 13, Step: 1610, Loss: 0.482265\n",
      "Epoch: 13, Step: 1620, Loss: 0.505833\n",
      "Epoch: 13, Step: 1630, Loss: 0.496937\n",
      "Epoch: 13, Step: 1640, Loss: 0.465041\n",
      "Epoch: 13, Step: 1650, Loss: 0.46334\n",
      "Epoch: 13, Step: 1660, Loss: 0.460259\n",
      "Epoch: 13, Step: 1670, Loss: 0.458491\n",
      "Epoch: 13, Step: 1680, Loss: 0.456236\n",
      "Epoch: 13, Step: 1690, Loss: 0.455683\n",
      "Epoch: 13, Step: 1700, Loss: 0.450407\n",
      "Epoch: 13, Step: 1710, Loss: 0.448867\n",
      "Epoch: 13, Step: 1720, Loss: 0.449431\n",
      "Epoch: 13, Step: 1730, Loss: 0.444996\n",
      "Epoch: 13, Step: 1740, Loss: 0.443755\n",
      "Epoch: 13, Step: 1750, Loss: 0.440718\n",
      "Epoch: 14, Step: 1400, Loss: 0.440293\n",
      "Epoch: 14, Step: 1410, Loss: 0.444804\n",
      "Epoch: 14, Step: 1420, Loss: 0.607015\n",
      "Epoch: 14, Step: 1430, Loss: 0.566824\n",
      "Epoch: 14, Step: 1440, Loss: 0.463904\n",
      "Epoch: 14, Step: 1450, Loss: 5.79928\n",
      "Epoch: 14, Step: 1460, Loss: 0.901676\n",
      "Epoch: 14, Step: 1470, Loss: 0.427537\n",
      "Epoch: 14, Step: 1480, Loss: 0.442234\n",
      "Epoch: 14, Step: 1490, Loss: 0.466313\n",
      "Epoch: 14, Step: 1500, Loss: 0.474613\n",
      "Epoch: 14, Step: 1510, Loss: 0.427776\n",
      "Epoch: 14, Step: 1520, Loss: 0.492262\n",
      "Epoch: 14, Step: 1530, Loss: 0.493557\n",
      "Epoch: 14, Step: 1540, Loss: 0.413962\n",
      "Epoch: 14, Step: 1550, Loss: 0.462408\n",
      "Epoch: 14, Step: 1560, Loss: 0.420386\n",
      "Epoch: 14, Step: 1570, Loss: 0.422736\n",
      "Epoch: 14, Step: 1580, Loss: 0.415412\n",
      "Epoch: 14, Step: 1590, Loss: 0.551728\n",
      "Epoch: 14, Step: 1600, Loss: 0.435536\n",
      "Epoch: 14, Step: 1610, Loss: 0.535241\n",
      "Epoch: 14, Step: 1620, Loss: 0.471442\n",
      "Epoch: 14, Step: 1630, Loss: 0.428433\n",
      "Epoch: 14, Step: 1640, Loss: 0.410632\n",
      "Epoch: 14, Step: 1650, Loss: 0.409168\n",
      "Epoch: 14, Step: 1660, Loss: 0.409601\n",
      "Epoch: 14, Step: 1670, Loss: 0.411075\n",
      "Epoch: 14, Step: 1680, Loss: 0.407995\n",
      "Epoch: 14, Step: 1690, Loss: 0.403946\n",
      "Epoch: 14, Step: 1700, Loss: 0.396998\n",
      "Epoch: 14, Step: 1710, Loss: 0.405082\n",
      "Epoch: 14, Step: 1720, Loss: 0.394214\n",
      "Epoch: 14, Step: 1730, Loss: 0.491399\n",
      "Epoch: 14, Step: 1740, Loss: 0.644859\n",
      "Epoch: 14, Step: 1750, Loss: 0.47629\n",
      "Epoch: 14, Step: 1760, Loss: 0.455362\n",
      "Epoch: 14, Step: 1770, Loss: 0.882317\n",
      "Epoch: 14, Step: 1780, Loss: 0.417323\n",
      "Epoch: 14, Step: 1790, Loss: 0.415587\n",
      "Epoch: 14, Step: 1800, Loss: 0.422658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Step: 1810, Loss: 0.691535\n",
      "Epoch: 14, Step: 1820, Loss: 0.398924\n",
      "Epoch: 14, Step: 1830, Loss: 0.475093\n",
      "Epoch: 14, Step: 1840, Loss: 0.590675\n",
      "Epoch: 14, Step: 1850, Loss: 0.544561\n",
      "Epoch: 15, Step: 1500, Loss: 0.390921\n",
      "Epoch: 15, Step: 1510, Loss: 0.385618\n",
      "Epoch: 15, Step: 1520, Loss: 0.363416\n",
      "Epoch: 15, Step: 1530, Loss: 0.395163\n",
      "Epoch: 15, Step: 1540, Loss: 0.532367\n",
      "Epoch: 15, Step: 1550, Loss: 1.03594\n",
      "Epoch: 15, Step: 1560, Loss: 0.653522\n",
      "Epoch: 15, Step: 1570, Loss: 1.22948\n",
      "Epoch: 15, Step: 1580, Loss: 0.677197\n",
      "Epoch: 15, Step: 1590, Loss: 0.852118\n",
      "Epoch: 15, Step: 1600, Loss: 0.622649\n",
      "Epoch: 15, Step: 1610, Loss: 0.354945\n",
      "Epoch: 15, Step: 1620, Loss: 0.354533\n",
      "Epoch: 15, Step: 1630, Loss: 0.379246\n",
      "Epoch: 15, Step: 1640, Loss: 0.453709\n",
      "Epoch: 15, Step: 1650, Loss: 1.21183\n",
      "Epoch: 15, Step: 1660, Loss: 0.481863\n",
      "Epoch: 15, Step: 1670, Loss: 2.13383\n",
      "Epoch: 15, Step: 1680, Loss: 0.819647\n",
      "Epoch: 15, Step: 1690, Loss: 1.21451\n",
      "Epoch: 15, Step: 1700, Loss: 0.511904\n",
      "Epoch: 15, Step: 1710, Loss: 0.486723\n",
      "Epoch: 15, Step: 1720, Loss: 0.441779\n",
      "Epoch: 15, Step: 1730, Loss: 1.33498\n",
      "Epoch: 15, Step: 1740, Loss: 0.991321\n",
      "Epoch: 15, Step: 1750, Loss: 0.4349\n",
      "Epoch: 15, Step: 1760, Loss: 0.364696\n",
      "Epoch: 15, Step: 1770, Loss: 0.399538\n",
      "Epoch: 15, Step: 1780, Loss: 0.726241\n",
      "Epoch: 15, Step: 1790, Loss: 0.526282\n",
      "Epoch: 15, Step: 1800, Loss: 1.34247\n",
      "Epoch: 15, Step: 1810, Loss: 0.984124\n",
      "Epoch: 15, Step: 1820, Loss: 0.360079\n",
      "Epoch: 15, Step: 1830, Loss: 0.323254\n",
      "Epoch: 15, Step: 1840, Loss: 0.330242\n",
      "Epoch: 15, Step: 1850, Loss: 0.33609\n",
      "Epoch: 15, Step: 1860, Loss: 0.365732\n",
      "Epoch: 15, Step: 1870, Loss: 0.877081\n",
      "Epoch: 15, Step: 1880, Loss: 0.822218\n",
      "Epoch: 15, Step: 1890, Loss: 0.450861\n",
      "Epoch: 15, Step: 1900, Loss: 0.339633\n",
      "Epoch: 15, Step: 1910, Loss: 0.321573\n",
      "Epoch: 15, Step: 1920, Loss: 0.332887\n",
      "Epoch: 15, Step: 1930, Loss: 0.609368\n",
      "Epoch: 15, Step: 1940, Loss: 1.68324\n",
      "Epoch: 15, Step: 1950, Loss: 0.997416\n",
      "Epoch: 16, Step: 1600, Loss: 0.361152\n",
      "Epoch: 16, Step: 1610, Loss: 0.336921\n",
      "Epoch: 16, Step: 1620, Loss: 0.299102\n",
      "Epoch: 16, Step: 1630, Loss: 0.294767\n",
      "Epoch: 16, Step: 1640, Loss: 0.302702\n",
      "Epoch: 16, Step: 1650, Loss: 0.308398\n",
      "Epoch: 16, Step: 1660, Loss: 0.373024\n",
      "Epoch: 16, Step: 1670, Loss: 0.304135\n",
      "Epoch: 16, Step: 1680, Loss: 0.375988\n",
      "Epoch: 16, Step: 1690, Loss: 0.507965\n",
      "Epoch: 16, Step: 1700, Loss: 0.60868\n",
      "Epoch: 16, Step: 1710, Loss: 0.540086\n",
      "Epoch: 16, Step: 1720, Loss: 0.310183\n",
      "Epoch: 16, Step: 1730, Loss: 0.282927\n",
      "Epoch: 16, Step: 1740, Loss: 0.282576\n",
      "Epoch: 16, Step: 1750, Loss: 0.281031\n",
      "Epoch: 16, Step: 1760, Loss: 0.306957\n",
      "Epoch: 16, Step: 1770, Loss: 1.54913\n",
      "Epoch: 16, Step: 1780, Loss: 4.45981\n",
      "Epoch: 16, Step: 1790, Loss: 0.290719\n",
      "Epoch: 16, Step: 1800, Loss: 0.276981\n",
      "Epoch: 16, Step: 1810, Loss: 0.274318\n",
      "Epoch: 16, Step: 1820, Loss: 0.277692\n",
      "Epoch: 16, Step: 1830, Loss: 0.274508\n",
      "Epoch: 16, Step: 1840, Loss: 0.27288\n",
      "Epoch: 16, Step: 1850, Loss: 0.274374\n",
      "Epoch: 16, Step: 1860, Loss: 0.271388\n",
      "Epoch: 16, Step: 1870, Loss: 0.273335\n",
      "Epoch: 16, Step: 1880, Loss: 0.26896\n",
      "Epoch: 16, Step: 1890, Loss: 0.273718\n",
      "Epoch: 16, Step: 1900, Loss: 0.284529\n",
      "Epoch: 16, Step: 1910, Loss: 0.281882\n",
      "Epoch: 16, Step: 1920, Loss: 0.271573\n",
      "Epoch: 16, Step: 1930, Loss: 0.262076\n",
      "Epoch: 16, Step: 1940, Loss: 0.265873\n",
      "Epoch: 16, Step: 1950, Loss: 0.264392\n",
      "Epoch: 16, Step: 1960, Loss: 0.269128\n",
      "Epoch: 16, Step: 1970, Loss: 0.262072\n",
      "Epoch: 16, Step: 1980, Loss: 0.309727\n",
      "Epoch: 16, Step: 1990, Loss: 0.338911\n",
      "Epoch: 16, Step: 2000, Loss: 0.394358\n",
      "Epoch: 16, Step: 2010, Loss: 0.430835\n",
      "Epoch: 16, Step: 2020, Loss: 0.367857\n",
      "Epoch: 16, Step: 2030, Loss: 0.271212\n",
      "Epoch: 16, Step: 2040, Loss: 0.295173\n",
      "Epoch: 16, Step: 2050, Loss: 0.284576\n",
      "Epoch: 17, Step: 1700, Loss: 0.257126\n",
      "Epoch: 17, Step: 1710, Loss: 0.347689\n",
      "Epoch: 17, Step: 1720, Loss: 0.340301\n",
      "Epoch: 17, Step: 1730, Loss: 3.92301\n",
      "Epoch: 17, Step: 1740, Loss: 2.07581\n",
      "Epoch: 17, Step: 1750, Loss: 0.252918\n",
      "Epoch: 17, Step: 1760, Loss: 0.248153\n",
      "Epoch: 17, Step: 1770, Loss: 0.293801\n",
      "Epoch: 17, Step: 1780, Loss: 0.298465\n",
      "Epoch: 17, Step: 1790, Loss: 0.267676\n",
      "Epoch: 17, Step: 1800, Loss: 0.301323\n",
      "Epoch: 17, Step: 1810, Loss: 0.320717\n",
      "Epoch: 17, Step: 1820, Loss: 0.24313\n",
      "Epoch: 17, Step: 1830, Loss: 0.265486\n",
      "Epoch: 17, Step: 1840, Loss: 0.260352\n",
      "Epoch: 17, Step: 1850, Loss: 0.254728\n",
      "Epoch: 17, Step: 1860, Loss: 0.240266\n",
      "Epoch: 17, Step: 1870, Loss: 0.312877\n",
      "Epoch: 17, Step: 1880, Loss: 0.269644\n",
      "Epoch: 17, Step: 1890, Loss: 0.316138\n",
      "Epoch: 17, Step: 1900, Loss: 0.291542\n",
      "Epoch: 17, Step: 1910, Loss: 0.284764\n",
      "Epoch: 17, Step: 1920, Loss: 0.254501\n",
      "Epoch: 17, Step: 1930, Loss: 0.240486\n",
      "Epoch: 17, Step: 1940, Loss: 0.29275\n",
      "Epoch: 17, Step: 1950, Loss: 0.519906\n",
      "Epoch: 17, Step: 1960, Loss: 0.288003\n",
      "Epoch: 17, Step: 1970, Loss: 0.293385\n",
      "Epoch: 17, Step: 1980, Loss: 0.326696\n",
      "Epoch: 17, Step: 1990, Loss: 0.235854\n",
      "Epoch: 17, Step: 2000, Loss: 0.237085\n",
      "Epoch: 17, Step: 2010, Loss: 0.265088\n",
      "Epoch: 17, Step: 2020, Loss: 0.301428\n",
      "Epoch: 17, Step: 2030, Loss: 0.24446\n",
      "Epoch: 17, Step: 2040, Loss: 0.269367\n",
      "Epoch: 17, Step: 2050, Loss: 0.520797\n",
      "Epoch: 17, Step: 2060, Loss: 0.245185\n",
      "Epoch: 17, Step: 2070, Loss: 0.256481\n",
      "Epoch: 17, Step: 2080, Loss: 0.288255\n",
      "Epoch: 17, Step: 2090, Loss: 0.558826\n",
      "Epoch: 17, Step: 2100, Loss: 0.32619\n",
      "Epoch: 17, Step: 2110, Loss: 0.268481\n",
      "Epoch: 17, Step: 2120, Loss: 0.426414\n",
      "Epoch: 17, Step: 2130, Loss: 0.453763\n",
      "Epoch: 17, Step: 2140, Loss: 0.228912\n",
      "Epoch: 17, Step: 2150, Loss: 0.268023\n",
      "Epoch: 18, Step: 1800, Loss: 0.219536\n",
      "Epoch: 18, Step: 1810, Loss: 0.250759\n",
      "Epoch: 18, Step: 1820, Loss: 0.267553\n",
      "Epoch: 18, Step: 1830, Loss: 0.979068\n",
      "Epoch: 18, Step: 1840, Loss: 0.369175\n",
      "Epoch: 18, Step: 1850, Loss: 0.932983\n",
      "Epoch: 18, Step: 1860, Loss: 0.645011\n",
      "Epoch: 18, Step: 1870, Loss: 0.545881\n",
      "Epoch: 18, Step: 1880, Loss: 0.542685\n",
      "Epoch: 18, Step: 1890, Loss: 0.238641\n",
      "Epoch: 18, Step: 1900, Loss: 0.227124\n",
      "Epoch: 18, Step: 1910, Loss: 0.325651\n",
      "Epoch: 18, Step: 1920, Loss: 0.31839\n",
      "Epoch: 18, Step: 1930, Loss: 1.05726\n",
      "Epoch: 18, Step: 1940, Loss: 0.62895\n",
      "Epoch: 18, Step: 1950, Loss: 1.41101\n",
      "Epoch: 18, Step: 1960, Loss: 0.79604\n",
      "Epoch: 18, Step: 1970, Loss: 1.17925\n",
      "Epoch: 18, Step: 1980, Loss: 0.432536\n",
      "Epoch: 18, Step: 1990, Loss: 0.221744\n",
      "Epoch: 18, Step: 2000, Loss: 0.296612\n",
      "Epoch: 18, Step: 2010, Loss: 1.42813\n",
      "Epoch: 18, Step: 2020, Loss: 1.21091\n",
      "Epoch: 18, Step: 2030, Loss: 0.275099\n",
      "Epoch: 18, Step: 2040, Loss: 0.265307\n",
      "Epoch: 18, Step: 2050, Loss: 0.243571\n",
      "Epoch: 18, Step: 2060, Loss: 0.630743\n",
      "Epoch: 18, Step: 2070, Loss: 0.349831\n",
      "Epoch: 18, Step: 2080, Loss: 1.10303\n",
      "Epoch: 18, Step: 2090, Loss: 1.00391\n",
      "Epoch: 18, Step: 2100, Loss: 0.352559\n",
      "Epoch: 18, Step: 2110, Loss: 0.202641\n",
      "Epoch: 18, Step: 2120, Loss: 0.209075\n",
      "Epoch: 18, Step: 2130, Loss: 0.214935\n",
      "Epoch: 18, Step: 2140, Loss: 0.235134\n",
      "Epoch: 18, Step: 2150, Loss: 0.655845\n",
      "Epoch: 18, Step: 2160, Loss: 0.694583\n",
      "Epoch: 18, Step: 2170, Loss: 0.403456\n",
      "Epoch: 18, Step: 2180, Loss: 0.232165\n",
      "Epoch: 18, Step: 2190, Loss: 0.223407\n",
      "Epoch: 18, Step: 2200, Loss: 0.214081\n",
      "Epoch: 18, Step: 2210, Loss: 0.276715\n",
      "Epoch: 18, Step: 2220, Loss: 0.995822\n",
      "Epoch: 18, Step: 2230, Loss: 1.05506\n",
      "Epoch: 18, Step: 2240, Loss: 0.227735\n",
      "Epoch: 18, Step: 2250, Loss: 0.347862\n",
      "Epoch: 19, Step: 1900, Loss: 0.284146\n",
      "Epoch: 19, Step: 1910, Loss: 0.236975\n",
      "Epoch: 19, Step: 1920, Loss: 0.213287\n",
      "Epoch: 19, Step: 1930, Loss: 0.210821\n",
      "Epoch: 19, Step: 1940, Loss: 0.245663\n",
      "Epoch: 19, Step: 1950, Loss: 0.199944\n",
      "Epoch: 19, Step: 1960, Loss: 0.223474\n",
      "Epoch: 19, Step: 1970, Loss: 0.308072\n",
      "Epoch: 19, Step: 1980, Loss: 0.567349\n",
      "Epoch: 19, Step: 1990, Loss: 0.464723\n",
      "Epoch: 19, Step: 2000, Loss: 0.216633\n",
      "Epoch: 19, Step: 2010, Loss: 0.188796\n",
      "Epoch: 19, Step: 2020, Loss: 0.191151\n",
      "Epoch: 19, Step: 2030, Loss: 0.19287\n",
      "Epoch: 19, Step: 2040, Loss: 0.190997\n",
      "Epoch: 19, Step: 2050, Loss: 0.519248\n",
      "Epoch: 19, Step: 2060, Loss: 5.19421\n",
      "Epoch: 19, Step: 2070, Loss: 0.239174\n",
      "Epoch: 19, Step: 2080, Loss: 0.189752\n",
      "Epoch: 19, Step: 2090, Loss: 0.182327\n",
      "Epoch: 19, Step: 2100, Loss: 0.185343\n",
      "Epoch: 19, Step: 2110, Loss: 0.185556\n",
      "Epoch: 19, Step: 2120, Loss: 0.182007\n",
      "Epoch: 19, Step: 2130, Loss: 0.195813\n",
      "Epoch: 19, Step: 2140, Loss: 0.189458\n",
      "Epoch: 19, Step: 2150, Loss: 0.194965\n",
      "Epoch: 19, Step: 2160, Loss: 0.191627\n",
      "Epoch: 19, Step: 2170, Loss: 0.216574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Step: 2180, Loss: 0.202954\n",
      "Epoch: 19, Step: 2190, Loss: 0.319332\n",
      "Epoch: 19, Step: 2200, Loss: 0.436632\n",
      "Epoch: 19, Step: 2210, Loss: 0.295458\n",
      "Epoch: 19, Step: 2220, Loss: 0.312463\n",
      "Epoch: 19, Step: 2230, Loss: 0.208031\n",
      "Epoch: 19, Step: 2240, Loss: 0.193495\n",
      "Epoch: 19, Step: 2250, Loss: 0.19795\n",
      "Epoch: 19, Step: 2260, Loss: 0.300564\n",
      "Epoch: 19, Step: 2270, Loss: 0.20896\n",
      "Epoch: 19, Step: 2280, Loss: 0.218525\n",
      "Epoch: 19, Step: 2290, Loss: 0.215582\n",
      "Epoch: 19, Step: 2300, Loss: 0.229545\n",
      "Epoch: 19, Step: 2310, Loss: 0.211023\n",
      "Epoch: 19, Step: 2320, Loss: 0.196407\n",
      "Epoch: 19, Step: 2330, Loss: 0.179001\n",
      "Epoch: 19, Step: 2340, Loss: 0.173156\n",
      "Epoch: 19, Step: 2350, Loss: 0.184691\n",
      "Epoch: 20, Step: 2000, Loss: 0.269477\n",
      "Epoch: 20, Step: 2010, Loss: 2.66926\n",
      "Epoch: 20, Step: 2020, Loss: 3.1814\n",
      "Epoch: 20, Step: 2030, Loss: 0.180999\n",
      "Epoch: 20, Step: 2040, Loss: 0.170422\n",
      "Epoch: 20, Step: 2050, Loss: 0.219316\n",
      "Epoch: 20, Step: 2060, Loss: 0.214738\n",
      "Epoch: 20, Step: 2070, Loss: 0.212591\n",
      "Epoch: 20, Step: 2080, Loss: 0.208048\n",
      "Epoch: 20, Step: 2090, Loss: 0.248478\n",
      "Epoch: 20, Step: 2100, Loss: 0.175632\n",
      "Epoch: 20, Step: 2110, Loss: 0.186178\n",
      "Epoch: 20, Step: 2120, Loss: 0.18769\n",
      "Epoch: 20, Step: 2130, Loss: 0.20883\n",
      "Epoch: 20, Step: 2140, Loss: 0.195057\n",
      "Epoch: 20, Step: 2150, Loss: 0.223679\n",
      "Epoch: 20, Step: 2160, Loss: 0.334586\n",
      "Epoch: 20, Step: 2170, Loss: 0.26147\n",
      "Epoch: 20, Step: 2180, Loss: 0.250155\n",
      "Epoch: 20, Step: 2190, Loss: 0.19106\n",
      "Epoch: 20, Step: 2200, Loss: 0.201679\n",
      "Epoch: 20, Step: 2210, Loss: 0.173938\n",
      "Epoch: 20, Step: 2220, Loss: 0.17172\n",
      "Epoch: 20, Step: 2230, Loss: 0.17545\n",
      "Epoch: 20, Step: 2240, Loss: 0.179037\n",
      "Epoch: 20, Step: 2250, Loss: 0.178372\n",
      "Epoch: 20, Step: 2260, Loss: 0.177519\n",
      "Epoch: 20, Step: 2270, Loss: 0.188853\n",
      "Epoch: 20, Step: 2280, Loss: 0.177931\n",
      "Epoch: 20, Step: 2290, Loss: 0.179363\n",
      "Epoch: 20, Step: 2300, Loss: 0.166618\n",
      "Epoch: 20, Step: 2310, Loss: 0.19683\n",
      "Epoch: 20, Step: 2320, Loss: 0.235369\n",
      "Epoch: 20, Step: 2330, Loss: 0.412362\n",
      "Epoch: 20, Step: 2340, Loss: 0.219515\n",
      "Epoch: 20, Step: 2350, Loss: 0.215104\n",
      "Epoch: 20, Step: 2360, Loss: 0.254256\n",
      "Epoch: 20, Step: 2370, Loss: 0.391448\n",
      "Epoch: 20, Step: 2380, Loss: 0.340342\n",
      "Epoch: 20, Step: 2390, Loss: 0.177988\n",
      "Epoch: 20, Step: 2400, Loss: 0.346363\n",
      "Epoch: 20, Step: 2410, Loss: 0.379223\n",
      "Epoch: 20, Step: 2420, Loss: 0.226353\n",
      "Epoch: 20, Step: 2430, Loss: 0.215227\n",
      "Epoch: 20, Step: 2440, Loss: 0.160832\n",
      "Epoch: 20, Step: 2450, Loss: 0.194905\n",
      "Epoch: 21, Step: 2100, Loss: 0.179498\n",
      "Epoch: 21, Step: 2110, Loss: 0.721601\n",
      "Epoch: 21, Step: 2120, Loss: 0.48141\n",
      "Epoch: 21, Step: 2130, Loss: 0.451706\n",
      "Epoch: 21, Step: 2140, Loss: 0.603112\n",
      "Epoch: 21, Step: 2150, Loss: 0.344516\n",
      "Epoch: 21, Step: 2160, Loss: 0.695534\n",
      "Epoch: 21, Step: 2170, Loss: 0.216212\n",
      "Epoch: 21, Step: 2180, Loss: 0.179959\n",
      "Epoch: 21, Step: 2190, Loss: 0.170231\n",
      "Epoch: 21, Step: 2200, Loss: 0.249998\n",
      "Epoch: 21, Step: 2210, Loss: 0.473568\n",
      "Epoch: 21, Step: 2220, Loss: 0.717849\n",
      "Epoch: 21, Step: 2230, Loss: 1.37083\n",
      "Epoch: 21, Step: 2240, Loss: 1.48802\n",
      "Epoch: 21, Step: 2250, Loss: 1.03329\n",
      "Epoch: 21, Step: 2260, Loss: 0.592253\n",
      "Epoch: 21, Step: 2270, Loss: 0.15585\n",
      "Epoch: 21, Step: 2280, Loss: 0.194779\n",
      "Epoch: 21, Step: 2290, Loss: 1.06611\n",
      "Epoch: 21, Step: 2300, Loss: 1.48419\n",
      "Epoch: 21, Step: 2310, Loss: 0.203377\n",
      "Epoch: 21, Step: 2320, Loss: 0.225463\n",
      "Epoch: 21, Step: 2330, Loss: 0.162312\n",
      "Epoch: 21, Step: 2340, Loss: 0.447347\n",
      "Epoch: 21, Step: 2350, Loss: 0.394906\n",
      "Epoch: 21, Step: 2360, Loss: 0.789654\n",
      "Epoch: 21, Step: 2370, Loss: 0.90979\n",
      "Epoch: 21, Step: 2380, Loss: 0.365832\n",
      "Epoch: 21, Step: 2390, Loss: 0.157327\n",
      "Epoch: 21, Step: 2400, Loss: 0.157116\n",
      "Epoch: 21, Step: 2410, Loss: 0.164529\n",
      "Epoch: 21, Step: 2420, Loss: 0.273114\n",
      "Epoch: 21, Step: 2430, Loss: 0.368647\n",
      "Epoch: 21, Step: 2440, Loss: 0.801152\n",
      "Epoch: 21, Step: 2450, Loss: 0.241173\n",
      "Epoch: 21, Step: 2460, Loss: 0.232271\n",
      "Epoch: 21, Step: 2470, Loss: 0.179069\n",
      "Epoch: 21, Step: 2480, Loss: 0.157945\n",
      "Epoch: 21, Step: 2490, Loss: 0.241853\n",
      "Epoch: 21, Step: 2500, Loss: 0.89423\n",
      "Epoch: 21, Step: 2510, Loss: 1.27251\n",
      "Epoch: 21, Step: 2520, Loss: 0.223859\n",
      "Epoch: 21, Step: 2530, Loss: 0.257383\n",
      "Epoch: 21, Step: 2540, Loss: 0.154894\n",
      "Epoch: 21, Step: 2550, Loss: 0.153643\n",
      "Epoch: 22, Step: 2200, Loss: 0.155771\n",
      "Epoch: 22, Step: 2210, Loss: 0.160444\n",
      "Epoch: 22, Step: 2220, Loss: 0.180185\n",
      "Epoch: 22, Step: 2230, Loss: 0.183519\n",
      "Epoch: 22, Step: 2240, Loss: 0.154759\n",
      "Epoch: 22, Step: 2250, Loss: 0.284306\n",
      "Epoch: 22, Step: 2260, Loss: 0.563258\n",
      "Epoch: 22, Step: 2270, Loss: 0.287208\n",
      "Epoch: 22, Step: 2280, Loss: 0.334282\n",
      "Epoch: 22, Step: 2290, Loss: 0.161383\n",
      "Epoch: 22, Step: 2300, Loss: 0.159842\n",
      "Epoch: 22, Step: 2310, Loss: 0.160663\n",
      "Epoch: 22, Step: 2320, Loss: 0.172684\n",
      "Epoch: 22, Step: 2330, Loss: 0.209496\n",
      "Epoch: 22, Step: 2340, Loss: 5.36573\n",
      "Epoch: 22, Step: 2350, Loss: 0.366927\n",
      "Epoch: 22, Step: 2360, Loss: 0.16286\n",
      "Epoch: 22, Step: 2370, Loss: 0.166529\n",
      "Epoch: 22, Step: 2380, Loss: 0.161534\n",
      "Epoch: 22, Step: 2390, Loss: 0.211524\n",
      "Epoch: 22, Step: 2400, Loss: 0.641706\n",
      "Epoch: 22, Step: 2410, Loss: 0.554883\n",
      "Epoch: 22, Step: 2420, Loss: 0.173776\n",
      "Epoch: 22, Step: 2430, Loss: 0.23583\n",
      "Epoch: 22, Step: 2440, Loss: 0.172993\n",
      "Epoch: 22, Step: 2450, Loss: 0.167176\n",
      "Epoch: 22, Step: 2460, Loss: 0.226523\n",
      "Epoch: 22, Step: 2470, Loss: 0.385845\n",
      "Epoch: 22, Step: 2480, Loss: 0.21935\n",
      "Epoch: 22, Step: 2490, Loss: 0.168031\n",
      "Epoch: 22, Step: 2500, Loss: 0.146603\n",
      "Epoch: 22, Step: 2510, Loss: 0.146128\n",
      "Epoch: 22, Step: 2520, Loss: 0.146671\n",
      "Epoch: 22, Step: 2530, Loss: 0.144201\n",
      "Epoch: 22, Step: 2540, Loss: 0.14899\n",
      "Epoch: 22, Step: 2550, Loss: 0.143418\n",
      "Epoch: 22, Step: 2560, Loss: 0.145501\n",
      "Epoch: 22, Step: 2570, Loss: 0.145257\n",
      "Epoch: 22, Step: 2580, Loss: 0.144415\n",
      "Epoch: 22, Step: 2590, Loss: 0.143648\n",
      "Epoch: 22, Step: 2600, Loss: 0.143569\n",
      "Epoch: 22, Step: 2610, Loss: 0.142708\n",
      "Epoch: 22, Step: 2620, Loss: 0.143367\n",
      "Epoch: 22, Step: 2630, Loss: 0.149457\n",
      "Epoch: 22, Step: 2640, Loss: 0.225617\n",
      "Epoch: 22, Step: 2650, Loss: 1.2913\n",
      "Epoch: 23, Step: 2300, Loss: 4.55338\n",
      "Epoch: 23, Step: 2310, Loss: 0.158349\n",
      "Epoch: 23, Step: 2320, Loss: 0.179622\n",
      "Epoch: 23, Step: 2330, Loss: 0.205082\n",
      "Epoch: 23, Step: 2340, Loss: 0.164484\n",
      "Epoch: 23, Step: 2350, Loss: 0.168009\n",
      "Epoch: 23, Step: 2360, Loss: 0.171559\n",
      "Epoch: 23, Step: 2370, Loss: 0.290972\n",
      "Epoch: 23, Step: 2380, Loss: 0.166368\n",
      "Epoch: 23, Step: 2390, Loss: 0.147907\n",
      "Epoch: 23, Step: 2400, Loss: 0.173369\n",
      "Epoch: 23, Step: 2410, Loss: 0.166808\n",
      "Epoch: 23, Step: 2420, Loss: 0.149323\n",
      "Epoch: 23, Step: 2430, Loss: 0.168964\n",
      "Epoch: 23, Step: 2440, Loss: 0.172071\n",
      "Epoch: 23, Step: 2450, Loss: 0.200883\n",
      "Epoch: 23, Step: 2460, Loss: 0.208215\n",
      "Epoch: 23, Step: 2470, Loss: 0.163776\n",
      "Epoch: 23, Step: 2480, Loss: 0.188659\n",
      "Epoch: 23, Step: 2490, Loss: 0.150085\n",
      "Epoch: 23, Step: 2500, Loss: 0.146844\n",
      "Epoch: 23, Step: 2510, Loss: 0.150167\n",
      "Epoch: 23, Step: 2520, Loss: 0.153299\n",
      "Epoch: 23, Step: 2530, Loss: 0.155399\n",
      "Epoch: 23, Step: 2540, Loss: 0.150507\n",
      "Epoch: 23, Step: 2550, Loss: 0.147432\n",
      "Epoch: 23, Step: 2560, Loss: 0.146905\n",
      "Epoch: 23, Step: 2570, Loss: 0.154543\n",
      "Epoch: 23, Step: 2580, Loss: 0.14808\n",
      "Epoch: 23, Step: 2590, Loss: 0.149814\n",
      "Epoch: 23, Step: 2600, Loss: 0.183731\n",
      "Epoch: 23, Step: 2610, Loss: 0.307778\n",
      "Epoch: 23, Step: 2620, Loss: 0.303507\n",
      "Epoch: 23, Step: 2630, Loss: 0.207279\n",
      "Epoch: 23, Step: 2640, Loss: 0.213079\n",
      "Epoch: 23, Step: 2650, Loss: 0.249982\n",
      "Epoch: 23, Step: 2660, Loss: 0.29899\n",
      "Epoch: 23, Step: 2670, Loss: 0.145717\n",
      "Epoch: 23, Step: 2680, Loss: 0.276397\n",
      "Epoch: 23, Step: 2690, Loss: 0.376928\n",
      "Epoch: 23, Step: 2700, Loss: 1.39187\n",
      "Epoch: 23, Step: 2710, Loss: 0.682792\n",
      "Epoch: 23, Step: 2720, Loss: 0.148111\n",
      "Epoch: 23, Step: 2730, Loss: 0.16436\n",
      "Epoch: 23, Step: 2740, Loss: 0.151847\n",
      "Epoch: 23, Step: 2750, Loss: 0.698415\n",
      "Epoch: 24, Step: 2400, Loss: 0.503313\n",
      "Epoch: 24, Step: 2410, Loss: 0.663338\n",
      "Epoch: 24, Step: 2420, Loss: 0.725088\n",
      "Epoch: 24, Step: 2430, Loss: 0.223423\n",
      "Epoch: 24, Step: 2440, Loss: 0.4913\n",
      "Epoch: 24, Step: 2450, Loss: 0.224155\n",
      "Epoch: 24, Step: 2460, Loss: 0.152132\n",
      "Epoch: 24, Step: 2470, Loss: 0.158264\n",
      "Epoch: 24, Step: 2480, Loss: 0.24982\n",
      "Epoch: 24, Step: 2490, Loss: 0.301399\n",
      "Epoch: 24, Step: 2500, Loss: 0.860931\n",
      "Epoch: 24, Step: 2510, Loss: 0.68561\n",
      "Epoch: 24, Step: 2520, Loss: 1.87706\n",
      "Epoch: 24, Step: 2530, Loss: 0.702575\n",
      "Epoch: 24, Step: 2540, Loss: 0.698769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Step: 2550, Loss: 0.139437\n",
      "Epoch: 24, Step: 2560, Loss: 0.149627\n",
      "Epoch: 24, Step: 2570, Loss: 0.68561\n",
      "Epoch: 24, Step: 2580, Loss: 1.72759\n",
      "Epoch: 24, Step: 2590, Loss: 0.297786\n",
      "Epoch: 24, Step: 2600, Loss: 0.310453\n",
      "Epoch: 24, Step: 2610, Loss: 0.148698\n",
      "Epoch: 24, Step: 2620, Loss: 0.307076\n",
      "Epoch: 24, Step: 2630, Loss: 0.514519\n",
      "Epoch: 24, Step: 2640, Loss: 0.580294\n",
      "Epoch: 24, Step: 2650, Loss: 0.893253\n",
      "Epoch: 24, Step: 2660, Loss: 0.809091\n",
      "Epoch: 24, Step: 2670, Loss: 0.471758\n",
      "Epoch: 24, Step: 2680, Loss: 0.225957\n",
      "Epoch: 24, Step: 2690, Loss: 0.165332\n",
      "Epoch: 24, Step: 2700, Loss: 0.348097\n",
      "Epoch: 24, Step: 2710, Loss: 0.464848\n",
      "Epoch: 24, Step: 2720, Loss: 0.488831\n",
      "Epoch: 24, Step: 2730, Loss: 0.390975\n",
      "Epoch: 24, Step: 2740, Loss: 0.24208\n",
      "Epoch: 24, Step: 2750, Loss: 0.178004\n",
      "Epoch: 24, Step: 2760, Loss: 0.195333\n",
      "Epoch: 24, Step: 2770, Loss: 0.223971\n",
      "Epoch: 24, Step: 2780, Loss: 0.616804\n",
      "Epoch: 24, Step: 2790, Loss: 1.19492\n",
      "Epoch: 24, Step: 2800, Loss: 0.30731\n",
      "Epoch: 24, Step: 2810, Loss: 0.245538\n",
      "Epoch: 24, Step: 2820, Loss: 0.137017\n",
      "Epoch: 24, Step: 2830, Loss: 0.140017\n",
      "Epoch: 24, Step: 2840, Loss: 0.136426\n",
      "Epoch: 24, Step: 2850, Loss: 0.142165\n",
      "Epoch: 25, Step: 2500, Loss: 0.153072\n",
      "Epoch: 25, Step: 2510, Loss: 0.183012\n",
      "Epoch: 25, Step: 2520, Loss: 0.146937\n",
      "Epoch: 25, Step: 2530, Loss: 0.224305\n",
      "Epoch: 25, Step: 2540, Loss: 0.566197\n",
      "Epoch: 25, Step: 2550, Loss: 0.289341\n",
      "Epoch: 25, Step: 2560, Loss: 0.598326\n",
      "Epoch: 25, Step: 2570, Loss: 0.168541\n",
      "Epoch: 25, Step: 2580, Loss: 0.165663\n",
      "Epoch: 25, Step: 2590, Loss: 0.183655\n",
      "Epoch: 25, Step: 2600, Loss: 0.205386\n",
      "Epoch: 25, Step: 2610, Loss: 0.213768\n",
      "Epoch: 25, Step: 2620, Loss: 5.32541\n",
      "Epoch: 25, Step: 2630, Loss: 1.85533\n",
      "Epoch: 25, Step: 2640, Loss: 0.232564\n",
      "Epoch: 25, Step: 2650, Loss: 0.277499\n",
      "Epoch: 25, Step: 2660, Loss: 0.24734\n",
      "Epoch: 25, Step: 2670, Loss: 0.144917\n",
      "Epoch: 25, Step: 2680, Loss: 0.172295\n",
      "Epoch: 25, Step: 2690, Loss: 0.177096\n",
      "Epoch: 25, Step: 2700, Loss: 0.139069\n",
      "Epoch: 25, Step: 2710, Loss: 0.136229\n",
      "Epoch: 25, Step: 2720, Loss: 0.139124\n",
      "Epoch: 25, Step: 2730, Loss: 0.152968\n",
      "Epoch: 25, Step: 2740, Loss: 0.150079\n",
      "Epoch: 25, Step: 2750, Loss: 0.148124\n",
      "Epoch: 25, Step: 2760, Loss: 0.137604\n",
      "Epoch: 25, Step: 2770, Loss: 0.137941\n",
      "Epoch: 25, Step: 2780, Loss: 0.131521\n",
      "Epoch: 25, Step: 2790, Loss: 0.157586\n",
      "Epoch: 25, Step: 2800, Loss: 0.176973\n",
      "Epoch: 25, Step: 2810, Loss: 0.161738\n",
      "Epoch: 25, Step: 2820, Loss: 0.136587\n",
      "Epoch: 25, Step: 2830, Loss: 0.154049\n",
      "Epoch: 25, Step: 2840, Loss: 0.150136\n",
      "Epoch: 25, Step: 2850, Loss: 0.167417\n",
      "Epoch: 25, Step: 2860, Loss: 0.160338\n",
      "Epoch: 25, Step: 2870, Loss: 0.15907\n",
      "Epoch: 25, Step: 2880, Loss: 0.173346\n",
      "Epoch: 25, Step: 2890, Loss: 0.149864\n",
      "Epoch: 25, Step: 2900, Loss: 0.138852\n",
      "Epoch: 25, Step: 2910, Loss: 0.141977\n",
      "Epoch: 25, Step: 2920, Loss: 0.423652\n",
      "Epoch: 25, Step: 2930, Loss: 0.578936\n",
      "Epoch: 25, Step: 2940, Loss: 6.50451\n",
      "Epoch: 25, Step: 2950, Loss: 0.233704\n",
      "Epoch: 26, Step: 2600, Loss: 0.132029\n",
      "Epoch: 26, Step: 2610, Loss: 0.143996\n",
      "Epoch: 26, Step: 2620, Loss: 0.21456\n",
      "Epoch: 26, Step: 2630, Loss: 0.202744\n",
      "Epoch: 26, Step: 2640, Loss: 0.130711\n",
      "Epoch: 26, Step: 2650, Loss: 0.206807\n",
      "Epoch: 26, Step: 2660, Loss: 0.161273\n",
      "Epoch: 26, Step: 2670, Loss: 0.136157\n",
      "Epoch: 26, Step: 2680, Loss: 0.150299\n",
      "Epoch: 26, Step: 2690, Loss: 0.163256\n",
      "Epoch: 26, Step: 2700, Loss: 0.135203\n",
      "Epoch: 26, Step: 2710, Loss: 0.155408\n",
      "Epoch: 26, Step: 2720, Loss: 0.195947\n",
      "Epoch: 26, Step: 2730, Loss: 0.15606\n",
      "Epoch: 26, Step: 2740, Loss: 0.189187\n",
      "Epoch: 26, Step: 2750, Loss: 0.159927\n",
      "Epoch: 26, Step: 2760, Loss: 0.175276\n",
      "Epoch: 26, Step: 2770, Loss: 0.139864\n",
      "Epoch: 26, Step: 2780, Loss: 0.130423\n",
      "Epoch: 26, Step: 2790, Loss: 0.133874\n",
      "Epoch: 26, Step: 2800, Loss: 0.136753\n",
      "Epoch: 26, Step: 2810, Loss: 0.13991\n",
      "Epoch: 26, Step: 2820, Loss: 0.13674\n",
      "Epoch: 26, Step: 2830, Loss: 0.130738\n",
      "Epoch: 26, Step: 2840, Loss: 0.136437\n",
      "Epoch: 26, Step: 2850, Loss: 0.14309\n",
      "Epoch: 26, Step: 2860, Loss: 0.137485\n",
      "Epoch: 26, Step: 2870, Loss: 0.137308\n",
      "Epoch: 26, Step: 2880, Loss: 0.154552\n",
      "Epoch: 26, Step: 2890, Loss: 0.260887\n",
      "Epoch: 26, Step: 2900, Loss: 1.41002\n",
      "Epoch: 26, Step: 2910, Loss: 0.575694\n",
      "Epoch: 26, Step: 2920, Loss: 1.34913\n",
      "Epoch: 26, Step: 2930, Loss: 0.311135\n",
      "Epoch: 26, Step: 2940, Loss: 0.261085\n",
      "Epoch: 26, Step: 2950, Loss: 0.14513\n",
      "Epoch: 26, Step: 2960, Loss: 0.23857\n",
      "Epoch: 26, Step: 2970, Loss: 0.314249\n",
      "Epoch: 26, Step: 2980, Loss: 0.247964\n",
      "Epoch: 26, Step: 2990, Loss: 0.172021\n",
      "Epoch: 26, Step: 3000, Loss: 0.129536\n",
      "Epoch: 26, Step: 3010, Loss: 0.13797\n",
      "Epoch: 26, Step: 3020, Loss: 0.156037\n",
      "Epoch: 26, Step: 3030, Loss: 0.442508\n",
      "Epoch: 26, Step: 3040, Loss: 0.486042\n",
      "Epoch: 26, Step: 3050, Loss: 0.536395\n",
      "Epoch: 27, Step: 2700, Loss: 0.814318\n",
      "Epoch: 27, Step: 2710, Loss: 0.308623\n",
      "Epoch: 27, Step: 2720, Loss: 0.551332\n",
      "Epoch: 27, Step: 2730, Loss: 0.258625\n",
      "Epoch: 27, Step: 2740, Loss: 0.135173\n",
      "Epoch: 27, Step: 2750, Loss: 0.139316\n",
      "Epoch: 27, Step: 2760, Loss: 0.178379\n",
      "Epoch: 27, Step: 2770, Loss: 0.239314\n",
      "Epoch: 27, Step: 2780, Loss: 0.923995\n",
      "Epoch: 27, Step: 2790, Loss: 0.268417\n",
      "Epoch: 27, Step: 2800, Loss: 1.76477\n",
      "Epoch: 27, Step: 2810, Loss: 0.542037\n",
      "Epoch: 27, Step: 2820, Loss: 0.824758\n",
      "Epoch: 27, Step: 2830, Loss: 0.147742\n",
      "Epoch: 27, Step: 2840, Loss: 0.140523\n",
      "Epoch: 27, Step: 2850, Loss: 0.290528\n",
      "Epoch: 27, Step: 2860, Loss: 1.37048\n",
      "Epoch: 27, Step: 2870, Loss: 0.568475\n",
      "Epoch: 27, Step: 2880, Loss: 0.212859\n",
      "Epoch: 27, Step: 2890, Loss: 0.146791\n",
      "Epoch: 27, Step: 2900, Loss: 0.229872\n",
      "Epoch: 27, Step: 2910, Loss: 0.497645\n",
      "Epoch: 27, Step: 2920, Loss: 1.11651\n",
      "Epoch: 27, Step: 2930, Loss: 1.16884\n",
      "Epoch: 27, Step: 2940, Loss: 0.604197\n",
      "Epoch: 27, Step: 2950, Loss: 0.157931\n",
      "Epoch: 27, Step: 2960, Loss: 0.125537\n",
      "Epoch: 27, Step: 2970, Loss: 0.133137\n",
      "Epoch: 27, Step: 2980, Loss: 0.150074\n",
      "Epoch: 27, Step: 2990, Loss: 0.276257\n",
      "Epoch: 27, Step: 3000, Loss: 0.500719\n",
      "Epoch: 27, Step: 3010, Loss: 0.534009\n",
      "Epoch: 27, Step: 3020, Loss: 0.249977\n",
      "Epoch: 27, Step: 3030, Loss: 0.15503\n",
      "Epoch: 27, Step: 3040, Loss: 0.166588\n",
      "Epoch: 27, Step: 3050, Loss: 0.158528\n",
      "Epoch: 27, Step: 3060, Loss: 0.465337\n",
      "Epoch: 27, Step: 3070, Loss: 1.21906\n",
      "Epoch: 27, Step: 3080, Loss: 0.526578\n",
      "Epoch: 27, Step: 3090, Loss: 0.234832\n",
      "Epoch: 27, Step: 3100, Loss: 0.177562\n",
      "Epoch: 27, Step: 3110, Loss: 0.128827\n",
      "Epoch: 27, Step: 3120, Loss: 0.126308\n",
      "Epoch: 27, Step: 3130, Loss: 0.133847\n",
      "Epoch: 27, Step: 3140, Loss: 0.137307\n",
      "Epoch: 27, Step: 3150, Loss: 0.175698\n",
      "Epoch: 28, Step: 2800, Loss: 0.16495\n",
      "Epoch: 28, Step: 2810, Loss: 0.646899\n",
      "Epoch: 28, Step: 2820, Loss: 4.6017\n",
      "Epoch: 28, Step: 2830, Loss: 0.345345\n",
      "Epoch: 28, Step: 2840, Loss: 0.434583\n",
      "Epoch: 28, Step: 2850, Loss: 0.154272\n",
      "Epoch: 28, Step: 2860, Loss: 0.249504\n",
      "Epoch: 28, Step: 2870, Loss: 0.264915\n",
      "Epoch: 28, Step: 2880, Loss: 0.352827\n",
      "Epoch: 28, Step: 2890, Loss: 0.206422\n",
      "Epoch: 28, Step: 2900, Loss: 5.43502\n",
      "Epoch: 28, Step: 2910, Loss: 3.3627\n",
      "Epoch: 28, Step: 2920, Loss: 0.12907\n",
      "Epoch: 28, Step: 2930, Loss: 0.132238\n",
      "Epoch: 28, Step: 2940, Loss: 0.170757\n",
      "Epoch: 28, Step: 2950, Loss: 0.133548\n",
      "Epoch: 28, Step: 2960, Loss: 0.129267\n",
      "Epoch: 28, Step: 2970, Loss: 0.12934\n",
      "Epoch: 28, Step: 2980, Loss: 0.137816\n",
      "Epoch: 28, Step: 2990, Loss: 0.129929\n",
      "Epoch: 28, Step: 3000, Loss: 0.136307\n",
      "Epoch: 28, Step: 3010, Loss: 0.134551\n",
      "Epoch: 28, Step: 3020, Loss: 0.145819\n",
      "Epoch: 28, Step: 3030, Loss: 0.133855\n",
      "Epoch: 28, Step: 3040, Loss: 0.127806\n",
      "Epoch: 28, Step: 3050, Loss: 0.126664\n",
      "Epoch: 28, Step: 3060, Loss: 0.121234\n",
      "Epoch: 28, Step: 3070, Loss: 0.136431\n",
      "Epoch: 28, Step: 3080, Loss: 0.176641\n",
      "Epoch: 28, Step: 3090, Loss: 0.156733\n",
      "Epoch: 28, Step: 3100, Loss: 0.127894\n",
      "Epoch: 28, Step: 3110, Loss: 0.217061\n",
      "Epoch: 28, Step: 3120, Loss: 0.180061\n",
      "Epoch: 28, Step: 3130, Loss: 0.927012\n",
      "Epoch: 28, Step: 3140, Loss: 4.68334\n",
      "Epoch: 28, Step: 3150, Loss: 2.62349\n",
      "Epoch: 28, Step: 3160, Loss: 1.90883\n",
      "Epoch: 28, Step: 3170, Loss: 1.51465\n",
      "Epoch: 28, Step: 3180, Loss: 0.315367\n",
      "Epoch: 28, Step: 3190, Loss: 0.14028\n",
      "Epoch: 28, Step: 3200, Loss: 0.644775\n",
      "Epoch: 28, Step: 3210, Loss: 0.24998\n",
      "Epoch: 28, Step: 3220, Loss: 5.05423\n",
      "Epoch: 28, Step: 3230, Loss: 0.671697\n",
      "Epoch: 28, Step: 3240, Loss: 0.130233\n",
      "Epoch: 28, Step: 3250, Loss: 0.153925\n",
      "Epoch: 29, Step: 2900, Loss: 0.15252\n",
      "Epoch: 29, Step: 2910, Loss: 0.159472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Step: 2920, Loss: 0.128433\n",
      "Epoch: 29, Step: 2930, Loss: 0.176266\n",
      "Epoch: 29, Step: 2940, Loss: 0.155887\n",
      "Epoch: 29, Step: 2950, Loss: 0.132643\n",
      "Epoch: 29, Step: 2960, Loss: 0.152357\n",
      "Epoch: 29, Step: 2970, Loss: 0.142456\n",
      "Epoch: 29, Step: 2980, Loss: 0.132061\n",
      "Epoch: 29, Step: 2990, Loss: 0.12184\n",
      "Epoch: 29, Step: 3000, Loss: 0.195559\n",
      "Epoch: 29, Step: 3010, Loss: 0.148731\n",
      "Epoch: 29, Step: 3020, Loss: 0.201449\n",
      "Epoch: 29, Step: 3030, Loss: 0.154455\n",
      "Epoch: 29, Step: 3040, Loss: 0.212159\n",
      "Epoch: 29, Step: 3050, Loss: 0.158778\n",
      "Epoch: 29, Step: 3060, Loss: 0.13012\n",
      "Epoch: 29, Step: 3070, Loss: 0.135449\n",
      "Epoch: 29, Step: 3080, Loss: 0.13549\n",
      "Epoch: 29, Step: 3090, Loss: 0.266154\n",
      "Epoch: 29, Step: 3100, Loss: 2.30551\n",
      "Epoch: 29, Step: 3110, Loss: 7.89721\n",
      "Epoch: 29, Step: 3120, Loss: 3.81066\n",
      "Epoch: 29, Step: 3130, Loss: 0.14655\n",
      "Epoch: 29, Step: 3140, Loss: 0.132214\n",
      "Epoch: 29, Step: 3150, Loss: 0.132502\n",
      "Epoch: 29, Step: 3160, Loss: 0.142655\n",
      "Epoch: 29, Step: 3170, Loss: 0.132828\n",
      "Epoch: 29, Step: 3180, Loss: 0.514066\n",
      "Epoch: 29, Step: 3190, Loss: 0.249546\n",
      "Epoch: 29, Step: 3200, Loss: 0.272891\n",
      "Epoch: 29, Step: 3210, Loss: 0.250185\n",
      "Epoch: 29, Step: 3220, Loss: 0.391886\n",
      "Epoch: 29, Step: 3230, Loss: 0.167852\n",
      "Epoch: 29, Step: 3240, Loss: 0.187934\n",
      "Epoch: 29, Step: 3250, Loss: 0.213495\n",
      "Epoch: 29, Step: 3260, Loss: 0.260192\n",
      "Epoch: 29, Step: 3270, Loss: 0.141966\n",
      "Epoch: 29, Step: 3280, Loss: 0.140988\n",
      "Epoch: 29, Step: 3290, Loss: 0.127647\n",
      "Epoch: 29, Step: 3300, Loss: 0.154109\n",
      "Epoch: 29, Step: 3310, Loss: 0.288283\n",
      "Epoch: 29, Step: 3320, Loss: 0.753094\n",
      "Epoch: 29, Step: 3330, Loss: 0.31799\n",
      "Epoch: 29, Step: 3340, Loss: 0.728675\n",
      "Epoch: 29, Step: 3350, Loss: 0.69049\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=./logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "LOGDIR = './save'\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "\n",
    "train_vars = tf.trainable_variables()\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(y_tr, y_pred))) + tf.add_n([tf.nn.l2_loss(v) for v in train_vars]) * L2NormConst\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "# merge all summaries into a single op\n",
    "merged_summary_op =  tf.summary.merge_all()\n",
    "\n",
    "# op to write logs to Tensorboard\n",
    "logs_path = './logs'\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "# train over the dataset about 30 times\n",
    "for epoch in range(epochs):\n",
    "    for i in range(int(num_images/batch_size)):\n",
    "        xs, ys = LoadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict={x_in: xs, y_tr: ys, keep_prob: 0.5})\n",
    "        if i % 10 == 0:\n",
    "            xs, ys = LoadValBatch(batch_size)\n",
    "            loss_value = loss.eval(feed_dict={x_in:xs, y_tr: ys, keep_prob: 0.5})\n",
    "            print(\"Epoch: %d, Step: %d, Loss: %g\" % (epoch, epoch * batch_size + i, loss_value))\n",
    "\n",
    "    # write logs at every iteration\n",
    "    summary = merged_summary_op.eval(feed_dict={x_in:xs, y_tr: ys, keep_prob: 0.5})\n",
    "    summary_writer.add_summary(summary, epoch * num_images/batch_size + i)\n",
    "\n",
    "    if i % batch_size == 0:\n",
    "        if not os.path.exists(LOGDIR):\n",
    "            os.makedirs(LOGDIR)\n",
    "            checkpoint_path = os.path.join(LOGDIR, \"model.ckpt\")\n",
    "            filename = saver.save(sess, checkpoint_path)\n",
    "            print(\"Model saved in file: %s\" % filename)\n",
    "            \n",
    "    if (i+1) % batch_size == 0:\n",
    "        if not os.path.exists(SAVEDIR):\n",
    "            os.makedirs(SAVEDIR)\n",
    "        save_path = os.path.join(SAVEDIR, \"model.ckpt\")\n",
    "        saver.save(sess = sess, save_path = save_path)\n",
    "        print(\"Model saved at location {} at epoch {}\".format(save_path, epoch + 1))\n",
    "\n",
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=./logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hims1\\\\Autopilot-TensorFlow-master\\\\Autopilot-TensorFlow-master'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.misc\n",
    "import model\n",
    "import cv2\n",
    "from subprocess import call\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"save/model.ckpt\")\n",
    "\n",
    "img = cv2.imread('steering_wheel_image.jpg',0)\n",
    "rows,cols = img.shape\n",
    "\n",
    "smoothed_angle = 0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(cv2.waitKey(10) != ord('q')):\n",
    "    ret, frame = cap.read()\n",
    "    image = scipy.misc.imresize(frame, [66, 200]) / 255.0\n",
    "    degrees = model.y.eval(feed_dict={model.x: [image], model.keep_prob: 1.0})[0][0] * 180 / scipy.pi\n",
    "    call(\"clear\")\n",
    "    print(\"Predicted steering angle: \" + str(degrees) + \" degrees\")\n",
    "    cv2.imshow('frame', frame)\n",
    "    #make smooth angle transitions by turning the steering wheel based on the difference of the current angle\n",
    "    #and the predicted angle\n",
    "    smoothed_angle += 0.2 * pow(abs((degrees - smoothed_angle)), 2.0 / 3.0) * (degrees - smoothed_angle) / abs(degrees - smoothed_angle)\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),-smoothed_angle,1)\n",
    "    dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "    cv2.imshow(\"steering wheel\", dst)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XHFnthirwlfn"
   },
   "outputs": [],
   "source": [
    "# Credits: https://github.com/SullyChen/Autopilot-TensorFlow\n",
    "# Research paper: End to End Learning for Self-Driving Cars by Nvidia. [https://arxiv.org/pdf/1604.07316.pdf]\n",
    "\n",
    "# NVidia dataset: 72 hrs of video => 72*60*60*30 = 7,776,000 images\n",
    "# Nvidia blog: https://devblogs.nvidia.com/deep-learning-self-driving-cars/\n",
    "\n",
    "\n",
    "# Our Dataset: https://github.com/SullyChen/Autopilot-TensorFlow [https://drive.google.com/file/d/0B-KJCaaF7elleG1RbzVPZWV4Tlk/view]\n",
    "# Size: 25 minutes = 25*60*30 = 45,000 images ~ 2.3 GB\n",
    "\n",
    "\n",
    "# If you want to try on a slightly large dataset: 70 minutes of data ~ 223GB\n",
    "# Refer: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5\n",
    "# Format: Image, latitude, longitude, gear, brake, throttle, steering angles and speed\n",
    "\n",
    "\n",
    "\n",
    "# Additional Installations:\n",
    "# pip3 install h5py\n",
    "\n",
    "\n",
    "# AWS: https://aws.amazon.com/blogs/machine-learning/get-started-with-deep-learning-using-the-aws-deep-learning-ami/\n",
    "\n",
    "# Youtube:https://www.youtube.com/watch?v=qhUvQiKec2U\n",
    "# Further reading and extensions: https://medium.com/udacity/teaching-a-machine-to-steer-a-car-d73217f2492c\n",
    "# More data: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Self_driving_car.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
